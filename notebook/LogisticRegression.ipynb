{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6cef907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import Modules/Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "import warnings\n",
    "#suppress warnings with numpy for sigmoid function\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2f2ba7",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df612c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train = pd.read_csv('../datasets/clean/cleaned_train.csv', index_col = False)\n",
    "test = pd.read_csv('../datasets/clean/cleaned_test.csv', index_col = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d55aac2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Astronomy</th>\n",
       "      <th>Herbology</th>\n",
       "      <th>Defense Against the Dark Arts</th>\n",
       "      <th>Divination</th>\n",
       "      <th>Muggle Studies</th>\n",
       "      <th>Ancient Runes</th>\n",
       "      <th>History of Magic</th>\n",
       "      <th>Transfiguration</th>\n",
       "      <th>Charms</th>\n",
       "      <th>Flying</th>\n",
       "      <th>Gryffindor</th>\n",
       "      <th>Hufflepuff</th>\n",
       "      <th>Ravenclaw</th>\n",
       "      <th>Slytherin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.014194</td>\n",
       "      <td>0.878628</td>\n",
       "      <td>1.010346</td>\n",
       "      <td>0.377371</td>\n",
       "      <td>1.021139</td>\n",
       "      <td>0.345639</td>\n",
       "      <td>0.512444</td>\n",
       "      <td>0.219633</td>\n",
       "      <td>1.204553</td>\n",
       "      <td>-0.500330</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.137535</td>\n",
       "      <td>-1.365690</td>\n",
       "      <td>1.133455</td>\n",
       "      <td>-2.109573</td>\n",
       "      <td>-0.540256</td>\n",
       "      <td>-1.204191</td>\n",
       "      <td>0.258503</td>\n",
       "      <td>0.653769</td>\n",
       "      <td>-1.002983</td>\n",
       "      <td>-1.386928</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.780078</td>\n",
       "      <td>1.261379</td>\n",
       "      <td>0.776671</td>\n",
       "      <td>0.718622</td>\n",
       "      <td>1.828915</td>\n",
       "      <td>1.005195</td>\n",
       "      <td>0.133871</td>\n",
       "      <td>1.314249</td>\n",
       "      <td>1.825184</td>\n",
       "      <td>0.086673</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Astronomy  Herbology  Defense Against the Dark Arts  Divination  \\\n",
       "0  -1.014194   0.878628                       1.010346    0.377371   \n",
       "1  -1.137535  -1.365690                       1.133455   -2.109573   \n",
       "2  -0.780078   1.261379                       0.776671    0.718622   \n",
       "\n",
       "   Muggle Studies  Ancient Runes  History of Magic  Transfiguration    Charms  \\\n",
       "0        1.021139       0.345639          0.512444         0.219633  1.204553   \n",
       "1       -0.540256      -1.204191          0.258503         0.653769 -1.002983   \n",
       "2        1.828915       1.005195          0.133871         1.314249  1.825184   \n",
       "\n",
       "     Flying  Gryffindor  Hufflepuff  Ravenclaw  Slytherin  \n",
       "0 -0.500330           0           0          1          0  \n",
       "1 -1.386928           0           0          0          1  \n",
       "2  0.086673           0           0          1          0  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c7696fc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Astronomy</th>\n",
       "      <th>Herbology</th>\n",
       "      <th>Defense Against the Dark Arts</th>\n",
       "      <th>Divination</th>\n",
       "      <th>Muggle Studies</th>\n",
       "      <th>Ancient Runes</th>\n",
       "      <th>History of Magic</th>\n",
       "      <th>Transfiguration</th>\n",
       "      <th>Charms</th>\n",
       "      <th>Flying</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.261390</td>\n",
       "      <td>0.360013</td>\n",
       "      <td>-1.260955</td>\n",
       "      <td>1.165280</td>\n",
       "      <td>-0.289017</td>\n",
       "      <td>-0.965420</td>\n",
       "      <td>0.281215</td>\n",
       "      <td>0.377241</td>\n",
       "      <td>-0.126065</td>\n",
       "      <td>-0.364411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.789243</td>\n",
       "      <td>0.349486</td>\n",
       "      <td>0.785819</td>\n",
       "      <td>0.768919</td>\n",
       "      <td>1.536298</td>\n",
       "      <td>1.007714</td>\n",
       "      <td>0.790127</td>\n",
       "      <td>0.406955</td>\n",
       "      <td>1.375558</td>\n",
       "      <td>-0.493877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.539127</td>\n",
       "      <td>-1.403671</td>\n",
       "      <td>-0.540051</td>\n",
       "      <td>0.352583</td>\n",
       "      <td>-0.833736</td>\n",
       "      <td>0.868643</td>\n",
       "      <td>-1.927101</td>\n",
       "      <td>-2.122591</td>\n",
       "      <td>-1.095106</td>\n",
       "      <td>1.825147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Astronomy  Herbology  Defense Against the Dark Arts  Divination  \\\n",
       "0   1.261390   0.360013                      -1.260955    1.165280   \n",
       "1  -0.789243   0.349486                       0.785819    0.768919   \n",
       "2   0.539127  -1.403671                      -0.540051    0.352583   \n",
       "\n",
       "   Muggle Studies  Ancient Runes  History of Magic  Transfiguration    Charms  \\\n",
       "0       -0.289017      -0.965420          0.281215         0.377241 -0.126065   \n",
       "1        1.536298       1.007714          0.790127         0.406955  1.375558   \n",
       "2       -0.833736       0.868643         -1.927101        -2.122591 -1.095106   \n",
       "\n",
       "     Flying  \n",
       "0 -0.364411  \n",
       "1 -0.493877  \n",
       "2  1.825147  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d6890b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split X (independent variables) with the target value Y\n",
    "target_columns = ['Gryffindor', 'Hufflepuff', 'Ravenclaw', 'Slytherin']\n",
    "X_train = np.array(train.drop(columns = target_columns))\n",
    "y_train = np.array(train[target_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "14b813db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X_train is: (1600, 10)\n",
      "The shape of y_train is: (1600, 4)\n",
      "We have m = 1600 training examples\n"
     ]
    }
   ],
   "source": [
    "print ('The shape of X_train is: ' + str(X_train.shape))\n",
    "print ('The shape of y_train is: ' + str(y_train.shape))\n",
    "print ('We have m = %d training examples' % (len(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a5eee7bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.01419368,  0.87862839,  1.01034582,  0.37737107,  1.02113913,\n",
       "        0.34563861,  0.51244445,  0.21963306,  1.20455291, -0.50032993])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "35fa8110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 0, 0])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To Access the ith column \n",
    "y_train[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c91377",
   "metadata": {},
   "source": [
    "### Sigmoid function\n",
    "\n",
    "For logistic regression, the model is represented as\n",
    "\n",
    "$$ f_{\\mathbf{w},b}(x) = g(\\mathbf{w}\\cdot \\mathbf{x} + b)$$\n",
    "where function $g$ is the sigmoid function. The sigmoid function is defined as:\n",
    "\n",
    "$$g(z) = \\frac{1}{1+e^{-z}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8684fd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "    Args:\n",
    "        z (ndarray): A scalar, numpy array of any size.\n",
    "    Returns:\n",
    "        g (ndarray): sigmoid(z), with the same shape as z\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b33279a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid(0) = 0.5\n"
     ]
    }
   ],
   "source": [
    "print (\"sigmoid(0) = \" + str(sigmoid(0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0013e206",
   "metadata": {},
   "source": [
    "### Cost function for logistic regression\n",
    "\n",
    "\n",
    "Recall that for logistic regression, the cost function is of the form \n",
    "\n",
    "$$ J(\\mathbf{w},b) = \\frac{1}{m}\\sum_{i=0}^{m-1} \\left[ loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) \\right] \\tag{1}$$\n",
    "\n",
    "where\n",
    "* m is the number of training examples in the dataset\n",
    "\n",
    "\n",
    "* $loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)})$ is the cost for a single data point, which is - \n",
    "\n",
    "    $$loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = (-y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\tag{2}$$\n",
    "    \n",
    "    \n",
    "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ is the model's prediction, while $y^{(i)}$, which is the actual label\n",
    "\n",
    "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(\\mathbf{w} \\cdot \\mathbf{x^{(i)}} + b)$ where function $g$ is the sigmoid function.\n",
    "    * It might be helpful to first calculate an intermediate variable $z_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x^{(i)}} + b = w_0x^{(i)}_0 + ... + w_{n-1}x^{(i)}_{n-1} + b$ where $n$ is the number of features, before calculating $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(z_{\\mathbf{w},b}(\\mathbf{x}^{(i)}))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "0cfc6cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, w, b, lambda_= 1):\n",
    "    \"\"\"\n",
    "    Computes the cost over all examples\n",
    "    Args:\n",
    "      X : (ndarray Shape (m,n)) data, m examples by n features\n",
    "      y : (array_like Shape (m,)) target value \n",
    "      w : (array_like Shape (n,)) Values of parameters of the model      \n",
    "      b : scalar Values of bias parameter of the model\n",
    "      lambda_: unused placeholder\n",
    "    Returns:\n",
    "      total_cost: (scalar)         cost \n",
    "    \"\"\"\n",
    "\n",
    "    m, n = X.shape\n",
    "    \n",
    "    cost = 0\n",
    "    for i in range(m):\n",
    "        z = np.dot(X[i],w) + b\n",
    "        f_wb = sigmoid(z)\n",
    "        cost += -y[i]*np.log(f_wb) - (1-y[i])*np.log(1-f_wb)\n",
    "    total_cost = cost/m\n",
    "    \n",
    "\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "319de5ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1-y_train[:, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "19649090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at initial w (zeros): 0.693\n"
     ]
    }
   ],
   "source": [
    "m, n = X_train.shape\n",
    "\n",
    "# Compute and display cost with w initialized to zeroes\n",
    "initial_w = np.zeros(n)\n",
    "initial_b = 0.\n",
    "cost = compute_cost(X_train, y_train[:, 0], initial_w, initial_b)\n",
    "print('Cost at initial w (zeros): {:.3f}'.format(cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4c4d1e",
   "metadata": {},
   "source": [
    "###  Gradient for logistic regression\n",
    "\n",
    "In this section, you will implement the gradient for logistic regression.\n",
    "\n",
    "Recall that the gradient descent algorithm is:\n",
    "\n",
    "$$\\begin{align*}& \\text{repeat until convergence:} \\; \\lbrace \\newline \\; & b := b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b} \\newline       \\; & w_j := w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{1}  \\; & \\text{for j := 0..n-1}\\newline & \\rbrace\\end{align*}$$\n",
    "\n",
    "where, parameters $b$, $w_j$ are all updated simultaniously\n",
    "\n",
    "compute $\\frac{\\partial J(\\mathbf{w},b)}{\\partial w}$, $\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}$ from equations (2) and (3) below.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - \\mathbf{y}^{(i)}) \\tag{2}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - \\mathbf{y}^{(i)})x_{j}^{(i)} \\tag{3}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "821a0605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, w, b, lambda_=None): \n",
    "    \"\"\"\n",
    "    Computes the gradient for logistic regression \n",
    " \n",
    "    Args:\n",
    "      X : (ndarray Shape (m,n)) variable such as house size \n",
    "      y : (array_like Shape (m,1)) actual value \n",
    "      w : (array_like Shape (n,1)) values of parameters of the model      \n",
    "      b : (scalar)                 value of parameter of the model \n",
    "      lambda_: unused placeholder.\n",
    "    Returns\n",
    "      dj_dw: (array_like Shape (n,1)) The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db: (scalar)                The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    dj_dw = np.zeros(w.shape)\n",
    "    dj_db = 0.\n",
    "\n",
    "    z = np.dot(X, w) + b\n",
    "    f_wb = sigmoid(z)\n",
    "    \n",
    "    for j in range(n):\n",
    "        dj_dw[j] = (np.sum(np.dot(f_wb - y, X.T[j])))\n",
    "            \n",
    "    dj_dw = dj_dw / m\n",
    "    dj_db = np.sum(f_wb - y) / m\n",
    "        \n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d194133",
   "metadata": {},
   "source": [
    "### Learning parameters using gradient descent \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "22f0b27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters, lambda_): \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn theta. Updates theta by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      X :    (array_like Shape (m, n)\n",
    "      y :    (array_like Shape (m,))\n",
    "      w_in : (array_like Shape (n,))  Initial values of parameters of the model\n",
    "      b_in : (scalar)                 Initial value of parameter of the model\n",
    "      cost_function:                  function to compute cost\n",
    "      alpha : (float)                 Learning rate\n",
    "      num_iters : (int)               number of iterations to run gradient descent\n",
    "      lambda_ (scalar, float)         regularization constant\n",
    "      \n",
    "    Returns:\n",
    "      w : (array_like Shape (n,)) Updated values of parameters of the model after\n",
    "          running gradient descent\n",
    "      b : (scalar)                Updated value of parameter of the model after\n",
    "          running gradient descent\n",
    "    \"\"\"\n",
    "    \n",
    "    # number of training examples\n",
    "    m = len(X)\n",
    "    \n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    w_history = []\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "\n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_db, dj_dw = gradient_function(X, y, w_in, b_in, lambda_)   \n",
    "\n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        w_in = w_in - alpha * dj_dw\n",
    "        b_in = b_in - alpha * dj_db\n",
    "       \n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            cost =  cost_function(X, y, w_in, b_in, lambda_)\n",
    "            J_history.append(cost)\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters/10) == 0 or i == (num_iters-1):\n",
    "            w_history.append(w_in)\n",
    "            print(f\"Iteration {i:4}: Cost {float(J_history[-1]):8.2f}   \")\n",
    "        \n",
    "    return w_in, b_in, J_history, w_history #return w and J,w history for graphing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033d2dab",
   "metadata": {},
   "source": [
    "## Fit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d46a2882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost     1.50   \n",
      "Iteration 1000: Cost     0.09   \n",
      "Iteration 2000: Cost     0.07   \n",
      "Iteration 3000: Cost     0.06   \n",
      "Iteration 4000: Cost     0.06   \n",
      "Iteration 5000: Cost     0.06   \n",
      "Iteration 6000: Cost     0.06   \n",
      "Iteration 7000: Cost     0.06   \n",
      "Iteration 8000: Cost     0.06   \n",
      "Iteration 9000: Cost     0.06   \n",
      "Iteration 9999: Cost     0.05   \n"
     ]
    }
   ],
   "source": [
    "intial_w = 0.01 * (np.random.rand(n).reshape(-1,1) - 0.5)\n",
    "initial_b = -8\n",
    "\n",
    "\n",
    "# Some gradient descent settings\n",
    "iterations = 10000\n",
    "alpha = 0.01\n",
    "\n",
    "w,b, J_history,_ = gradient_descent(X_train ,y_train[:, 3], initial_w, initial_b, \n",
    "                                   compute_cost, compute_gradient, alpha, iterations, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8da5a11",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "27707f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, w, b): \n",
    "    \"\"\"\n",
    "    Predict whether the label is 0 or 1 using learned logistic\n",
    "    regression parameters w\n",
    "    \n",
    "    Args:\n",
    "    X : (ndarray Shape (m, n))\n",
    "    w : (array_like Shape (n,))      Parameters of the model\n",
    "    b : (scalar, float)              Parameter of the model\n",
    "\n",
    "    Returns:\n",
    "    p: (ndarray (m,1))\n",
    "        The predictions for X using a threshold at 0.5\n",
    "    \"\"\"\n",
    "    # number of training examples\n",
    "    m, n = X.shape\n",
    "    p = np.zeros(m)\n",
    "\n",
    "    f_wb = sigmoid(np.dot(X, w) + b)\n",
    "    for i, prob in enumerate(f_wb):\n",
    "        p[i] = 1 if prob >= 0.50 else 0\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d276ae8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 96.062500\n"
     ]
    }
   ],
   "source": [
    "#Compute accuracy on our training set\n",
    "p = predict(X_train, w,b)\n",
    "print('Train Accuracy: %f'%(np.mean(p == y_train[:, 3]) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710259b0",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b1a58e",
   "metadata": {},
   "source": [
    "# Make a class out of it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "8e3c1c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \"\"\"\n",
    "        A class to perform Logistic Regression\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "            Define attributes which will be passed later\n",
    "        \"\"\"\n",
    "        # train data and label\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        \n",
    "        # m: number of observations\n",
    "        self.m = None\n",
    "        \n",
    "        # n: number of independent variables (X)\n",
    "        self.n = None\n",
    "        \n",
    "        # store historic value of cost function. init as +infinity\n",
    "        self.costs = [np.inf]\n",
    "\n",
    "        self.J_history = []\n",
    "        self.w_history = []\n",
    "        \n",
    "        # parameteres (weights)\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        \n",
    "        # regularization constant lambda_ (scalar, float)\n",
    "        self.lambda_ = None\n",
    "\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"\n",
    "        Compute the sigmoid of z\n",
    "        Args:\n",
    "            z (ndarray): A scalar, numpy array of any size.\n",
    "        Returns:\n",
    "            g (ndarray): sigmoid(z), with the same shape as z\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    \n",
    "    def compute_cost(self, X, y, w, b):\n",
    "        \"\"\"\n",
    "        Computes the cost over all examples\n",
    "        Args:\n",
    "          X : (ndarray Shape (m,n)) data, m examples by n features\n",
    "          y : (array_like Shape (m,)) target value \n",
    "          w : (array_like Shape (n,)) Values of parameters of the model      \n",
    "          b : scalar Values of bias parameter of the model\n",
    "          lambda_: unused placeholder\n",
    "        Returns:\n",
    "          total_cost: (scalar)         cost \n",
    "        \"\"\"\n",
    "\n",
    "        m, n = X.shape\n",
    "\n",
    "        z = np.dot(X, w) + b\n",
    "        pred = self.sigmoid(z)\n",
    "\n",
    "        pred[pred == 1] = 1-1e-9 #hard cap max threshold\n",
    "        \n",
    "        \n",
    "        cost = np.dot(-y, np.log(pred)) - (np.dot(1 - y, np.log(1 - pred)))\n",
    "        total_cost = np.sum(cost) / m\n",
    "    \n",
    "        reg_cost = np.sum(np.dot(w, w.T))\n",
    "        total_cost = total_cost + (self.lambda_/(2 * m)) * reg_cost\n",
    "        \n",
    "        return total_cost\n",
    "\n",
    "    \n",
    "    def compute_gradient(self, X, y, w, b, lambda_): \n",
    "        \"\"\"\n",
    "        Computes the gradient for logistic regression \n",
    "\n",
    "        Args:\n",
    "          X : (ndarray Shape (m,n)) variable such as house size \n",
    "          y : (array_like Shape (m,1)) actual value \n",
    "          w : (array_like Shape (n,1)) values of parameters of the model      \n",
    "          b : (scalar)                 value of parameter of the model \n",
    "          lambda_: unused placeholder.\n",
    "        Returns\n",
    "          dj_dw: (array_like Shape (n,1)) The gradient of the cost w.r.t. the parameters w. \n",
    "          dj_db: (scalar)                The gradient of the cost w.r.t. the parameter b. \n",
    "        \"\"\"\n",
    "        m, n = X.shape\n",
    "        dj_dw = np.zeros(w.shape)\n",
    "        dj_db = 0.0\n",
    "\n",
    "        z = np.dot(X, w) + b\n",
    "        f_wb = self.sigmoid(z)\n",
    "\n",
    "        for j in range(n):\n",
    "            dj_dw[j] = (np.sum(np.dot(f_wb - y, X.T[j])))\n",
    "\n",
    "        dj_dw = dj_dw / m\n",
    "        dj_dw += np.dot((lambda_ / m), w) # add regularization\n",
    "        dj_db = np.sum(f_wb - y) / m\n",
    "\n",
    "        return dj_db, dj_dw\n",
    "\n",
    "\n",
    "    def gradient_descent(self, cost_function, gradient_function, alpha, num_iters, show_every): \n",
    "        \"\"\"\n",
    "        Performs batch gradient descent to learn theta. Updates theta by taking \n",
    "        num_iters gradient steps with learning rate alpha\n",
    "\n",
    "        Args:\n",
    "          X :    (array_like Shape (m, n)\n",
    "          y :    (array_like Shape (m,))\n",
    "          w_in : (array_like Shape (n,))  Initial values of parameters of the model\n",
    "          b_in : (scalar)                 Initial value of parameter of the model\n",
    "          cost_function:                  function to compute cost\n",
    "          alpha : (float)                 Learning rate\n",
    "          num_iters : (int)               number of iterations to run gradient descent\n",
    "          lambda_ (scalar, float)         regularization constant\n",
    "\n",
    "        Returns:\n",
    "          w : (array_like Shape (n,)) Updated values of parameters of the model after\n",
    "              running gradient descent\n",
    "          b : (scalar)                Updated value of parameter of the model after\n",
    "              running gradient descent\n",
    "        \"\"\"\n",
    "        \n",
    "        for i in range(num_iters):\n",
    "\n",
    "            # Calculate the gradient and update the parameters\n",
    "            dj_db, dj_dw = gradient_function(self.X, self.y, self.w, self.b, self.lambda_)   \n",
    "\n",
    "            # Update Parameters using w, b, alpha and gradient\n",
    "            self.w = self.w - alpha * dj_dw               \n",
    "            self.b = self.b - alpha * dj_db              \n",
    "\n",
    "            # Save cost J at each iteration\n",
    "            if i<100000:      # prevent resource exhaustion \n",
    "                cost =  cost_function(self.X, self.y, self.w, self.b)\n",
    "\n",
    "            # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "            if i % show_every == 0 or i == num_iters-1:\n",
    "                self.J_history.append(cost)\n",
    "#                 self.w_history.append(self.w)\n",
    "                print(f\"Iteration {i:4}: Cost {float(self.J_history[-1]):8.2f}   \")\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y, alpha=0.001, iterations=1500, show_every=None, lambda_= 1):\n",
    "        \"\"\"\n",
    "        setup attributes and apply training\n",
    "        \"\"\"\n",
    "        \n",
    "        # train data and label\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "        # m: number of observations\n",
    "        self.m, self.n = X.shape\n",
    "\n",
    "        # regularization coefficient\n",
    "#         self.reg = lambda_\n",
    "        self.lambda_ = lambda_\n",
    "        \n",
    "        # init weights if first call\n",
    "        if type(self.w) is not np.ndarray:\n",
    "            self.w = 0.01 * (np.random.rand(n).reshape(-1,1) - 0.5)\n",
    "        if self.b == None:\n",
    "            self.b = -8\n",
    "        if show_every == None:\n",
    "            show_every = iterations // 10 \n",
    "            \n",
    "        # Perform Gradient Descent\n",
    "        self.gradient_descent(self.compute_cost, self.compute_gradient, alpha, iterations, show_every)\n",
    "\n",
    "    \n",
    "    def predict(self, X): \n",
    "        \"\"\"\n",
    "        Predict whether the label is 0 or 1 using learned logistic\n",
    "        regression parameters w\n",
    "\n",
    "        Args:\n",
    "        X : (ndarray Shape (m, n))\n",
    "        w : (array_like Shape (n,))      Parameters of the model\n",
    "        b : (scalar, float)              Parameter of the model\n",
    "\n",
    "        Returns:\n",
    "        p: (ndarray (m,1))\n",
    "            The predictions for X using a threshold at 0.5\n",
    "        \"\"\"\n",
    "        \n",
    "        # Check if number of features matches our model\n",
    "        \n",
    "        \n",
    "        # number of training examples\n",
    "        m, n = X.shape\n",
    "        p = np.zeros(m)\n",
    "\n",
    "        f_wb = self.sigmoid(np.dot(X, self.w) + self.b)\n",
    "        for i, prob in enumerate(f_wb):\n",
    "            p[i] = 1 if prob >= 0.50 else 0\n",
    "        return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "cb2c0bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "88035472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost     1.61   \n",
      "Iteration   10: Cost     1.30   \n",
      "Iteration   20: Cost     1.00   \n",
      "Iteration   30: Cost     0.71   \n",
      "Iteration   40: Cost     0.44   \n",
      "Iteration   50: Cost     0.24   \n",
      "Iteration   60: Cost     0.13   \n",
      "Iteration   70: Cost     0.09   \n",
      "Iteration   80: Cost     0.08   \n",
      "Iteration   90: Cost     0.08   \n",
      "Iteration   99: Cost     0.08   \n"
     ]
    }
   ],
   "source": [
    "lr.fit(X_train, y_train[: , 0], alpha=0.00003, iterations=100, lambda_ = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "47f306b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost     0.08   \n",
      "Iteration   10: Cost     0.08   \n",
      "Iteration   20: Cost     0.08   \n",
      "Iteration   30: Cost     0.08   \n",
      "Iteration   40: Cost     0.08   \n",
      "Iteration   50: Cost     0.08   \n",
      "Iteration   60: Cost     0.08   \n",
      "Iteration   70: Cost     0.08   \n",
      "Iteration   80: Cost     0.08   \n",
      "Iteration   90: Cost     0.08   \n",
      "Iteration   99: Cost     0.08   \n"
     ]
    }
   ],
   "source": [
    "lr.fit(X_train, y_train[: , 0], alpha=0.000001, iterations=100, lambda_ = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "ed305b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 99.187500\n"
     ]
    }
   ],
   "source": [
    "#Compute accuracy on our training set\n",
    "p = lr.predict(X_train)\n",
    "print('Train Accuracy: %f'%(np.mean(p == y_train[:, 0]) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907531ca",
   "metadata": {},
   "source": [
    "#### Testing..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d88b883",
   "metadata": {},
   "source": [
    "# Support Multiclass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "4b45f4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultipleLogisticRegression:\n",
    "\n",
    "    def __init__(self):\n",
    "    \n",
    "        # C: number of categories for Y\n",
    "        # -> create C models and train them each one at teh time\n",
    "        self.c = None\n",
    "        \n",
    "        \n",
    "        self.models = []\n",
    "        \n",
    "    def softmax():\n",
    "        \"\"\"\n",
    "        takes the predicted values from the sub_models\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac94605",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
