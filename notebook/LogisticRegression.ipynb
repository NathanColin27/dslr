{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6cef907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import Modules/Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "import warnings\n",
    "#suppress warnings with numpy for sigmoid function\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2f2ba7",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df612c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train = pd.read_csv('../datasets/clean/cleaned_train.csv', index_col = False)\n",
    "test = pd.read_csv('../datasets/clean/cleaned_test.csv', index_col = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d55aac2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Astronomy</th>\n",
       "      <th>Herbology</th>\n",
       "      <th>Defense Against the Dark Arts</th>\n",
       "      <th>Divination</th>\n",
       "      <th>Muggle Studies</th>\n",
       "      <th>Ancient Runes</th>\n",
       "      <th>History of Magic</th>\n",
       "      <th>Transfiguration</th>\n",
       "      <th>Charms</th>\n",
       "      <th>Flying</th>\n",
       "      <th>Gryffindor</th>\n",
       "      <th>Hufflepuff</th>\n",
       "      <th>Ravenclaw</th>\n",
       "      <th>Slytherin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.014194</td>\n",
       "      <td>0.878628</td>\n",
       "      <td>1.010346</td>\n",
       "      <td>0.377371</td>\n",
       "      <td>1.021139</td>\n",
       "      <td>0.345639</td>\n",
       "      <td>0.512444</td>\n",
       "      <td>0.219633</td>\n",
       "      <td>1.204553</td>\n",
       "      <td>-0.500330</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.137535</td>\n",
       "      <td>-1.365690</td>\n",
       "      <td>1.133455</td>\n",
       "      <td>-2.109573</td>\n",
       "      <td>-0.540256</td>\n",
       "      <td>-1.204191</td>\n",
       "      <td>0.258503</td>\n",
       "      <td>0.653769</td>\n",
       "      <td>-1.002983</td>\n",
       "      <td>-1.386928</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.780078</td>\n",
       "      <td>1.261379</td>\n",
       "      <td>0.776671</td>\n",
       "      <td>0.718622</td>\n",
       "      <td>1.828915</td>\n",
       "      <td>1.005195</td>\n",
       "      <td>0.133871</td>\n",
       "      <td>1.314249</td>\n",
       "      <td>1.825184</td>\n",
       "      <td>0.086673</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Astronomy  Herbology  Defense Against the Dark Arts  Divination  \\\n",
       "0  -1.014194   0.878628                       1.010346    0.377371   \n",
       "1  -1.137535  -1.365690                       1.133455   -2.109573   \n",
       "2  -0.780078   1.261379                       0.776671    0.718622   \n",
       "\n",
       "   Muggle Studies  Ancient Runes  History of Magic  Transfiguration    Charms  \\\n",
       "0        1.021139       0.345639          0.512444         0.219633  1.204553   \n",
       "1       -0.540256      -1.204191          0.258503         0.653769 -1.002983   \n",
       "2        1.828915       1.005195          0.133871         1.314249  1.825184   \n",
       "\n",
       "     Flying  Gryffindor  Hufflepuff  Ravenclaw  Slytherin  \n",
       "0 -0.500330           0           0          1          0  \n",
       "1 -1.386928           0           0          0          1  \n",
       "2  0.086673           0           0          1          0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7696fc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Astronomy</th>\n",
       "      <th>Herbology</th>\n",
       "      <th>Defense Against the Dark Arts</th>\n",
       "      <th>Divination</th>\n",
       "      <th>Muggle Studies</th>\n",
       "      <th>Ancient Runes</th>\n",
       "      <th>History of Magic</th>\n",
       "      <th>Transfiguration</th>\n",
       "      <th>Charms</th>\n",
       "      <th>Flying</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.261390</td>\n",
       "      <td>0.360013</td>\n",
       "      <td>-1.260955</td>\n",
       "      <td>1.165280</td>\n",
       "      <td>-0.289017</td>\n",
       "      <td>-0.965420</td>\n",
       "      <td>0.281215</td>\n",
       "      <td>0.377241</td>\n",
       "      <td>-0.126065</td>\n",
       "      <td>-0.364411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.789243</td>\n",
       "      <td>0.349486</td>\n",
       "      <td>0.785819</td>\n",
       "      <td>0.768919</td>\n",
       "      <td>1.536298</td>\n",
       "      <td>1.007714</td>\n",
       "      <td>0.790127</td>\n",
       "      <td>0.406955</td>\n",
       "      <td>1.375558</td>\n",
       "      <td>-0.493877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.539127</td>\n",
       "      <td>-1.403671</td>\n",
       "      <td>-0.540051</td>\n",
       "      <td>0.352583</td>\n",
       "      <td>-0.833736</td>\n",
       "      <td>0.868643</td>\n",
       "      <td>-1.927101</td>\n",
       "      <td>-2.122591</td>\n",
       "      <td>-1.095106</td>\n",
       "      <td>1.825147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Astronomy  Herbology  Defense Against the Dark Arts  Divination  \\\n",
       "0   1.261390   0.360013                      -1.260955    1.165280   \n",
       "1  -0.789243   0.349486                       0.785819    0.768919   \n",
       "2   0.539127  -1.403671                      -0.540051    0.352583   \n",
       "\n",
       "   Muggle Studies  Ancient Runes  History of Magic  Transfiguration    Charms  \\\n",
       "0       -0.289017      -0.965420          0.281215         0.377241 -0.126065   \n",
       "1        1.536298       1.007714          0.790127         0.406955  1.375558   \n",
       "2       -0.833736       0.868643         -1.927101        -2.122591 -1.095106   \n",
       "\n",
       "     Flying  \n",
       "0 -0.364411  \n",
       "1 -0.493877  \n",
       "2  1.825147  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6890b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split X (independent variables) with the target value Y\n",
    "target_columns = ['Gryffindor', 'Hufflepuff', 'Ravenclaw', 'Slytherin']\n",
    "X_train = np.array(train.drop(columns = target_columns))\n",
    "y_train = np.array(train[target_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14b813db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X_train is: (1600, 10)\n",
      "The shape of y_train is: (1600, 4)\n",
      "We have m = 1600 training examples\n"
     ]
    }
   ],
   "source": [
    "print ('The shape of X_train is: ' + str(X_train.shape))\n",
    "print ('The shape of y_train is: ' + str(y_train.shape))\n",
    "print ('We have m = %d training examples' % (len(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5eee7bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.01419368,  0.87862839,  1.01034582,  0.37737107,  1.02113913,\n",
       "        0.34563861,  0.51244445,  0.21963306,  1.20455291, -0.50032993])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35fa8110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 0, 0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To Access the ith column \n",
    "y_train[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c91377",
   "metadata": {},
   "source": [
    "### Sigmoid function\n",
    "\n",
    "For logistic regression, the model is represented as\n",
    "\n",
    "$$ f_{\\mathbf{w},b}(x) = g(\\mathbf{w}\\cdot \\mathbf{x} + b)$$\n",
    "where function $g$ is the sigmoid function. The sigmoid function is defined as:\n",
    "\n",
    "$$g(z) = \\frac{1}{1+e^{-z}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8684fd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "    Args:\n",
    "        z (ndarray): A scalar, numpy array of any size.\n",
    "    Returns:\n",
    "        g (ndarray): sigmoid(z), with the same shape as z\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b33279a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid(0) = 0.5\n"
     ]
    }
   ],
   "source": [
    "print (\"sigmoid(0) = \" + str(sigmoid(0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0013e206",
   "metadata": {},
   "source": [
    "### Cost function for logistic regression\n",
    "\n",
    "\n",
    "Recall that for logistic regression, the cost function is of the form \n",
    "\n",
    "$$ J(\\mathbf{w},b) = \\frac{1}{m}\\sum_{i=0}^{m-1} \\left[ loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) \\right] \\tag{1}$$\n",
    "\n",
    "where\n",
    "* m is the number of training examples in the dataset\n",
    "\n",
    "\n",
    "* $loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)})$ is the cost for a single data point, which is - \n",
    "\n",
    "    $$loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = (-y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\tag{2}$$\n",
    "    \n",
    "    \n",
    "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ is the model's prediction, while $y^{(i)}$, which is the actual label\n",
    "\n",
    "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(\\mathbf{w} \\cdot \\mathbf{x^{(i)}} + b)$ where function $g$ is the sigmoid function.\n",
    "    * It might be helpful to first calculate an intermediate variable $z_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x^{(i)}} + b = w_0x^{(i)}_0 + ... + w_{n-1}x^{(i)}_{n-1} + b$ where $n$ is the number of features, before calculating $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(z_{\\mathbf{w},b}(\\mathbf{x}^{(i)}))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0cfc6cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, w, b, lambda_= 1):\n",
    "    \"\"\"\n",
    "    Computes the cost over all examples\n",
    "    Args:\n",
    "      X : (ndarray Shape (m,n)) data, m examples by n features\n",
    "      y : (array_like Shape (m,)) target value \n",
    "      w : (array_like Shape (n,)) Values of parameters of the model      \n",
    "      b : scalar Values of bias parameter of the model\n",
    "      lambda_: unused placeholder\n",
    "    Returns:\n",
    "      total_cost: (scalar)         cost \n",
    "    \"\"\"\n",
    "\n",
    "    m, n = X.shape\n",
    "    \n",
    "    cost = 0\n",
    "    for i in range(m):\n",
    "        z = np.dot(X[i],w) + b\n",
    "        f_wb = sigmoid(z)\n",
    "        cost += -y[i]*np.log(f_wb) - (1-y[i])*np.log(1-f_wb)\n",
    "    total_cost = cost/m\n",
    "    \n",
    "\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "319de5ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1-y_train[:, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19649090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at initial w (zeros): 0.693\n"
     ]
    }
   ],
   "source": [
    "m, n = X_train.shape\n",
    "\n",
    "# Compute and display cost with w initialized to zeroes\n",
    "initial_w = np.zeros(n)\n",
    "initial_b = 0.\n",
    "cost = compute_cost(X_train, y_train[:, 0], initial_w, initial_b)\n",
    "print('Cost at initial w (zeros): {:.3f}'.format(cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4c4d1e",
   "metadata": {},
   "source": [
    "###  Gradient for logistic regression\n",
    "\n",
    "In this section, you will implement the gradient for logistic regression.\n",
    "\n",
    "Recall that the gradient descent algorithm is:\n",
    "\n",
    "$$\\begin{align*}& \\text{repeat until convergence:} \\; \\lbrace \\newline \\; & b := b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b} \\newline       \\; & w_j := w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{1}  \\; & \\text{for j := 0..n-1}\\newline & \\rbrace\\end{align*}$$\n",
    "\n",
    "where, parameters $b$, $w_j$ are all updated simultaniously\n",
    "\n",
    "compute $\\frac{\\partial J(\\mathbf{w},b)}{\\partial w}$, $\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}$ from equations (2) and (3) below.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - \\mathbf{y}^{(i)}) \\tag{2}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - \\mathbf{y}^{(i)})x_{j}^{(i)} \\tag{3}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "821a0605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, w, b, lambda_=None): \n",
    "    \"\"\"\n",
    "    Computes the gradient for logistic regression \n",
    " \n",
    "    Args:\n",
    "      X : (ndarray Shape (m,n)) variable such as house size \n",
    "      y : (array_like Shape (m,1)) actual value \n",
    "      w : (array_like Shape (n,1)) values of parameters of the model      \n",
    "      b : (scalar)                 value of parameter of the model \n",
    "      lambda_: unused placeholder.\n",
    "    Returns\n",
    "      dj_dw: (array_like Shape (n,1)) The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db: (scalar)                The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    dj_dw = np.zeros(w.shape)\n",
    "    dj_db = 0.\n",
    "\n",
    "    z = np.dot(X, w) + b\n",
    "    f_wb = sigmoid(z)\n",
    "    \n",
    "    for j in range(n):\n",
    "        dj_dw[j] = (np.sum(np.dot(f_wb - y, X.T[j])))\n",
    "            \n",
    "    dj_dw = dj_dw / m\n",
    "    dj_db = np.sum(f_wb - y) / m\n",
    "        \n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d194133",
   "metadata": {},
   "source": [
    "### Learning parameters using gradient descent \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "22f0b27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters, lambda_): \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn theta. Updates theta by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      X :    (array_like Shape (m, n)\n",
    "      y :    (array_like Shape (m,))\n",
    "      w_in : (array_like Shape (n,))  Initial values of parameters of the model\n",
    "      b_in : (scalar)                 Initial value of parameter of the model\n",
    "      cost_function:                  function to compute cost\n",
    "      alpha : (float)                 Learning rate\n",
    "      num_iters : (int)               number of iterations to run gradient descent\n",
    "      lambda_ (scalar, float)         regularization constant\n",
    "      \n",
    "    Returns:\n",
    "      w : (array_like Shape (n,)) Updated values of parameters of the model after\n",
    "          running gradient descent\n",
    "      b : (scalar)                Updated value of parameter of the model after\n",
    "          running gradient descent\n",
    "    \"\"\"\n",
    "    \n",
    "    # number of training examples\n",
    "    m = len(X)\n",
    "    \n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    w_history = []\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "\n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_db, dj_dw = gradient_function(X, y, w_in, b_in, lambda_)   \n",
    "\n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        w_in = w_in - alpha * dj_dw\n",
    "        b_in = b_in - alpha * dj_db\n",
    "       \n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            cost =  cost_function(X, y, w_in, b_in, lambda_)\n",
    "            J_history.append(cost)\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters/10) == 0 or i == (num_iters-1):\n",
    "            w_history.append(w_in)\n",
    "            print(f\"Iteration {i:4}: Cost {float(J_history[-1]):8.2f}   \")\n",
    "        \n",
    "    return w_in, b_in, J_history, w_history #return w and J,w history for graphing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033d2dab",
   "metadata": {},
   "source": [
    "## Fit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d46a2882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost     0.12   \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [26], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m iterations \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000\u001b[39m\n\u001b[1;32m      8\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m\n\u001b[0;32m---> 10\u001b[0m w,b, J_history,_ \u001b[38;5;241m=\u001b[39m gradient_descent(X_train ,y_train[:, \u001b[38;5;241m3\u001b[39m], initial_w, initial_b, \n\u001b[1;32m     11\u001b[0m                                    compute_cost, compute_gradient, alpha, iterations, \u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[0;32mIn [23], line 41\u001b[0m, in \u001b[0;36mgradient_descent\u001b[0;34m(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters, lambda_)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Save cost J at each iteration\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m100000\u001b[39m:      \u001b[38;5;66;03m# prevent resource exhaustion \u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     cost \u001b[38;5;241m=\u001b[39m  \u001b[43mcost_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     J_history\u001b[38;5;241m.\u001b[39mappend(cost)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Print cost every at intervals 10 times or as many iterations if < 10\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [19], line 18\u001b[0m, in \u001b[0;36mcompute_cost\u001b[0;34m(X, y, w, b, lambda_)\u001b[0m\n\u001b[1;32m     16\u001b[0m cost \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(m):\n\u001b[0;32m---> 18\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m b\n\u001b[1;32m     19\u001b[0m     f_wb \u001b[38;5;241m=\u001b[39m sigmoid(z)\n\u001b[1;32m     20\u001b[0m     cost \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39my[i]\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mlog(f_wb) \u001b[38;5;241m-\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39my[i])\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mf_wb)\n",
      "File \u001b[0;32m<__array_function__ internals>:179\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/Volumes/Storage/goinfre/cduvivie/dslr/.venv/lib/python3.8/site-packages/numpy/core/multiarray.py:736\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(a, b, out)\u001b[0m\n\u001b[1;32m    666\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;124;03m    result_type(*arrays_and_dtypes)\u001b[39;00m\n\u001b[1;32m    668\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    731\u001b[0m \n\u001b[1;32m    732\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    733\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arrays_and_dtypes\n\u001b[0;32m--> 736\u001b[0m \u001b[38;5;129m@array_function_from_c_func_and_dispatcher\u001b[39m(_multiarray_umath\u001b[38;5;241m.\u001b[39mdot)\n\u001b[1;32m    737\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdot\u001b[39m(a, b, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;124;03m    dot(a, b, out=None)\u001b[39;00m\n\u001b[1;32m    740\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m \n\u001b[1;32m    821\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    822\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, b, out)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "m, n = X_train.shape\n",
    "initial_w = 0.01 * (np.random.rand(n).reshape(-1,1) - 0.5)\n",
    "initial_b = -8\n",
    "\n",
    "\n",
    "# Some gradient descent settings\n",
    "iterations = 10000\n",
    "alpha = 0.01\n",
    "\n",
    "w,b, J_history,_ = gradient_descent(X_train ,y_train[:, 3], initial_w, initial_b, \n",
    "                                   compute_cost, compute_gradient, alpha, iterations, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8da5a11",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27707f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, w, b): \n",
    "    \"\"\"\n",
    "    Predict whether the label is 0 or 1 using learned logistic\n",
    "    regression parameters w\n",
    "    \n",
    "    Args:\n",
    "    X : (ndarray Shape (m, n))\n",
    "    w : (array_like Shape (n,))      Parameters of the model\n",
    "    b : (scalar, float)              Parameter of the model\n",
    "\n",
    "    Returns:\n",
    "    p: (ndarray (m,1))\n",
    "        The predictions for X using a threshold at 0.5\n",
    "    \"\"\"\n",
    "    # number of training examples\n",
    "    m, n = X.shape\n",
    "    p = np.zeros(m)\n",
    "\n",
    "    f_wb = sigmoid(np.dot(X, w) + b)\n",
    "    for i, prob in enumerate(f_wb):\n",
    "        p[i] = 1 if prob >= 0.50 else 0\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d276ae8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 99.250000\n"
     ]
    }
   ],
   "source": [
    "#Compute accuracy on our training set\n",
    "p = predict(X_train, w,b)\n",
    "print('Train Accuracy: %f'%(np.mean(p == y_train[:, 3]) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710259b0",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b1a58e",
   "metadata": {},
   "source": [
    "# Make a class out of it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8e3c1c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \"\"\"\n",
    "        A class to perform Logistic Regression\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "            Define attributes which will be passed later\n",
    "        \"\"\"\n",
    "        # train data and label\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        \n",
    "        # m: number of observations\n",
    "        self.m = None\n",
    "        \n",
    "        # n: number of independent variables (X)\n",
    "        self.n = None\n",
    "        \n",
    "        # store historic value of cost function. init as +infinity\n",
    "        self.costs = [np.inf]\n",
    "\n",
    "        self.J_history = []\n",
    "        self.w_history = []\n",
    "        \n",
    "        # parameteres (weights)\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        \n",
    "        # regularization constant lambda_ (scalar, float)\n",
    "        self.lambda_ = None\n",
    "\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"\n",
    "        Compute the sigmoid of z\n",
    "        Args:\n",
    "            z (ndarray): A scalar, numpy array of any size.\n",
    "        Returns:\n",
    "            g (ndarray): sigmoid(z), with the same shape as z\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    \n",
    "    def compute_cost(self, X, y, w, b):\n",
    "        \"\"\"\n",
    "        Computes the cost over all examples\n",
    "        Args:\n",
    "          X : (ndarray Shape (m,n)) data, m examples by n features\n",
    "          y : (array_like Shape (m,)) target value \n",
    "          w : (array_like Shape (n,)) Values of parameters of the model      \n",
    "          b : scalar Values of bias parameter of the model\n",
    "          lambda_: unused placeholder\n",
    "        Returns:\n",
    "          total_cost: (scalar)         cost \n",
    "        \"\"\"\n",
    "\n",
    "        m, n = X.shape\n",
    "\n",
    "        z = np.dot(X, w) + b\n",
    "        pred = self.sigmoid(z)\n",
    "\n",
    "        pred[pred == 1] = 1-1e-9 #hard cap max threshold\n",
    "        \n",
    "        \n",
    "        cost = np.dot(-y, np.log(pred)) - (np.dot(1 - y, np.log(1 - pred)))\n",
    "        total_cost = np.sum(cost) / m\n",
    "    \n",
    "        reg_cost = np.sum(np.dot(w, w.T))\n",
    "        total_cost = total_cost + (self.lambda_/(2 * m)) * reg_cost\n",
    "        \n",
    "        return total_cost\n",
    "\n",
    "    \n",
    "    def compute_gradient(self, X, y, w, b, lambda_): \n",
    "        \"\"\"\n",
    "        Computes the gradient for logistic regression \n",
    "\n",
    "        Args:\n",
    "          X : (ndarray Shape (m,n)) variable such as house size \n",
    "          y : (array_like Shape (m,1)) actual value \n",
    "          w : (array_like Shape (n,1)) values of parameters of the model      \n",
    "          b : (scalar)                 value of parameter of the model \n",
    "          lambda_: unused placeholder.\n",
    "        Returns\n",
    "          dj_dw: (array_like Shape (n,1)) The gradient of the cost w.r.t. the parameters w. \n",
    "          dj_db: (scalar)                The gradient of the cost w.r.t. the parameter b. \n",
    "        \"\"\"\n",
    "        m, n = X.shape\n",
    "        dj_dw = np.zeros(w.shape)\n",
    "        dj_db = 0.0\n",
    "\n",
    "        z = np.dot(X, w) + b\n",
    "        f_wb = self.sigmoid(z)\n",
    "\n",
    "        for j in range(n):\n",
    "            dj_dw[j] = (np.sum(np.dot(f_wb - y, X.T[j])))\n",
    "\n",
    "        dj_dw = dj_dw / m\n",
    "        dj_dw += np.dot((lambda_ / m), w) # add regularization\n",
    "        dj_db = np.sum(f_wb - y) / m\n",
    "\n",
    "        return dj_db, dj_dw\n",
    "\n",
    "\n",
    "    def gradient_descent(self, cost_function, gradient_function, alpha, num_iters, show_every): \n",
    "        \"\"\"\n",
    "        Performs batch gradient descent to learn theta. Updates theta by taking \n",
    "        num_iters gradient steps with learning rate alpha\n",
    "\n",
    "        Args:\n",
    "          X :    (array_like Shape (m, n)\n",
    "          y :    (array_like Shape (m,))\n",
    "          w_in : (array_like Shape (n,))  Initial values of parameters of the model\n",
    "          b_in : (scalar)                 Initial value of parameter of the model\n",
    "          cost_function:                  function to compute cost\n",
    "          alpha : (float)                 Learning rate\n",
    "          num_iters : (int)               number of iterations to run gradient descent\n",
    "          lambda_ (scalar, float)         regularization constant\n",
    "\n",
    "        Returns:\n",
    "          w : (array_like Shape (n,)) Updated values of parameters of the model after\n",
    "              running gradient descent\n",
    "          b : (scalar)                Updated value of parameter of the model after\n",
    "              running gradient descent\n",
    "        \"\"\"\n",
    "        \n",
    "        w_in = self.w\n",
    "        b_in = self.b\n",
    "        for i in range(num_iters):\n",
    "\n",
    "            # Calculate the gradient and update the parameters\n",
    "            dj_db, dj_dw = gradient_function(self.X, self.y, w_in, b_in, self.lambda_)   \n",
    "\n",
    "            # Update Parameters using w, b, alpha and gradient\n",
    "            w_in = w_in - alpha * dj_dw               \n",
    "            b_in = b_in - alpha * dj_db              \n",
    "\n",
    "            # Save cost J at each iteration\n",
    "            if i<100000:      # prevent resource exhaustion \n",
    "                cost =  cost_function(self.X, self.y, w_in, b_in)\n",
    "\n",
    "            # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "            if i % show_every == 0 or i == num_iters-1:\n",
    "                self.J_history.append(cost)\n",
    "#                 self.w_history.append(self.w)\n",
    "                print(f\"Iteration {i:4}: Cost {float(self.J_history[-1]):8.2f}   \")\n",
    "        self.w = w_in\n",
    "        self.b = b_in\n",
    "        \n",
    "    \n",
    "    \n",
    "    def fit(self, X, y, alpha=0.001, iterations=1500, show_every=None, lambda_= 1):\n",
    "        \"\"\"\n",
    "        setup attributes and apply training\n",
    "        \"\"\"\n",
    "        \n",
    "        # train data and label\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "        # m: number of observations\n",
    "        self.m, self.n = X.shape\n",
    "\n",
    "        # regularization coefficient\n",
    "        self.lambda_ = lambda_\n",
    "        \n",
    "        # init weights if first call\n",
    "        if type(self.w) is not np.ndarray:\n",
    "            self.w = 0.01 * (np.random.rand(n).reshape(-1,1) - 0.5)\n",
    "        if self.b == None:\n",
    "            self.b = -8\n",
    "        if show_every == None:\n",
    "            show_every = iterations // 10 \n",
    "            \n",
    "        # Perform Gradient Descent\n",
    "        self.gradient_descent(self.compute_cost, self.compute_gradient, alpha, iterations, show_every)\n",
    "\n",
    "    \n",
    "    def predict(self, X, decision_boundary = None): \n",
    "        \"\"\"\n",
    "        Predict whether the label is 0 or 1 using learned logistic\n",
    "        regression parameters w\n",
    "\n",
    "        Args:\n",
    "        X : (ndarray Shape (m, n))\n",
    "        w : (array_like Shape (n,))      Parameters of the model\n",
    "        b : (scalar, float)              Parameter of the model\n",
    "\n",
    "        Returns:\n",
    "        p: (ndarray (m,1))\n",
    "            The predictions for X using a threshold at 0.5\n",
    "        \"\"\"\n",
    "        \n",
    "        # Check if number of features matches our model\n",
    "        \n",
    "        \n",
    "        # number of training examples\n",
    "        m, n = X.shape\n",
    "        p = np.zeros(m)\n",
    "\n",
    "        f_wb = self.sigmoid(np.dot(X, self.w) + self.b)\n",
    "        \n",
    "        if decision_boundary != None:\n",
    "            for i, prob in enumerate(f_wb):\n",
    "                p[i] = 1 if prob >= decision_boundary else 0\n",
    "            return p\n",
    "        \n",
    "        return f_wb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cb2c0bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "88035472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost     1.61   \n",
      "Iteration   10: Cost     1.30   \n",
      "Iteration   20: Cost     1.00   \n",
      "Iteration   30: Cost     0.71   \n",
      "Iteration   40: Cost     0.44   \n",
      "Iteration   50: Cost     0.24   \n",
      "Iteration   60: Cost     0.13   \n",
      "Iteration   70: Cost     0.09   \n",
      "Iteration   80: Cost     0.08   \n",
      "Iteration   90: Cost     0.08   \n",
      "Iteration   99: Cost     0.08   \n"
     ]
    }
   ],
   "source": [
    "lr.fit(X_train, y_train[: , 0], alpha=0.00003, iterations=100, lambda_ = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "47f306b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost     0.08   \n",
      "Iteration   10: Cost     0.08   \n",
      "Iteration   20: Cost     0.08   \n",
      "Iteration   30: Cost     0.08   \n",
      "Iteration   40: Cost     0.08   \n",
      "Iteration   50: Cost     0.08   \n",
      "Iteration   60: Cost     0.09   \n",
      "Iteration   70: Cost     0.09   \n",
      "Iteration   80: Cost     0.09   \n",
      "Iteration   90: Cost     0.09   \n",
      "Iteration   99: Cost     0.09   \n"
     ]
    }
   ],
   "source": [
    "lr.fit(X_train, y_train[: , 0], alpha=0.000003, iterations=100, lambda_ = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ed305b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 99.187500\n"
     ]
    }
   ],
   "source": [
    "#Compute accuracy on our training set\n",
    "p = lr.predict(X_train, 0.5)\n",
    "print('Train Accuracy: %f'%(np.mean(p == y_train[:, 0]) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907531ca",
   "metadata": {},
   "source": [
    "#### Testing..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d88b883",
   "metadata": {},
   "source": [
    "# Support Multiclass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4b45f4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultipleLogisticRegression:\n",
    "\n",
    "    def __init__(self):\n",
    "    \n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        \n",
    "        # C: number of categories for Y\n",
    "        # -> create C models and train them each one at teh time\n",
    "        self.c = None\n",
    "        \n",
    "        self.models = []\n",
    "        \n",
    "    def softmax(self, X):\n",
    "        \"\"\"\n",
    "        takes the predicted values from the sub_models\n",
    "        \"\"\"\n",
    "        pred = []\n",
    "        for i in range(self.c):\n",
    "            pred.append(self.models[i].predict(X))\n",
    "        pred = np.array(pred)\n",
    "        res = []\n",
    "        for row in pred.T[0]:\n",
    "            # for each row (student), return the column (house) with the highest probability\n",
    "            res.append(np.argmax(row / np.sum(row)))\n",
    "        res = np.array(res)\n",
    "        return res\n",
    "    \n",
    "    def accuracy(self, pred, true_labels):\n",
    "        \"\"\"\n",
    "        calculate the accuracy of the model prediction given the true labels\n",
    "        \"\"\"\n",
    "        if pred.shape != true_labels.shape:\n",
    "            print('Error in the shape of predictions and true_labels')\n",
    "            return \n",
    "        accuracy = np.mean(pred == true_labels) * 100\n",
    "        print('Accuracy: %f'%(accuracy))\n",
    "        return accuracy\n",
    "    \n",
    "    def score_matrix(self, pred, true_labels):\n",
    "        \"\"\"\n",
    "        calculate scores matrix\n",
    "        \"\"\"\n",
    "        # Calculate overall precision\n",
    "        score_matrix = np.zeros((self.c, self.c))\n",
    "        for i in range(0, len(pred)):\n",
    "            pred_label = pred[i]\n",
    "            true_label = answer[i]\n",
    "            score_matrix[pred_label][true_label] += 1\n",
    "            \n",
    "        precision_per_class = []\n",
    "        for i in range(self.c):\n",
    "            precision_per_class.append(score_matrix[i][i] / score_matrix[i].sum())\n",
    "        \n",
    "        return score_matrix, precision_per_class\n",
    "        \n",
    "    def fit(self, X, y, alpha=0.00003, iterations=100, show_every=None, lambda_= 1):\n",
    "        \"\"\"\n",
    "        setup attributes and apply training\n",
    "        \"\"\"\n",
    "        \n",
    "        # train data and label\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "        # m: number of observations\n",
    "        self.m, self.n = X.shape\n",
    "        \n",
    "        # c: number of category\n",
    "        self.c = y.shape[1]\n",
    "        \n",
    "        \n",
    "        if show_every == None:\n",
    "            show_every = iterations // 10 \n",
    "        \n",
    "        # init models\n",
    "        if not self.models:\n",
    "            for i in range(self.c):\n",
    "                self.models.append(LogisticRegression())\n",
    "        \n",
    "        # train models\n",
    "        decision_boundary = 0.5\n",
    "        for i in range(self.c):\n",
    "            model = self.models[i]\n",
    "            print(f\"training model {i+1} with alpha= {0.00003}\")\n",
    "            model.fit(X_train, y_train[: , i], alpha=0.00003, iterations=100, lambda_ = 1)\n",
    "            p = model.predict(X_train, decision_boundary)\n",
    "            print('Train Accuracy: %f'%(np.mean(p == y_train[:, i]) * 100))\n",
    "            \n",
    "            print(f\"training model {i+1} with alpha= {0.00001}\")\n",
    "            model.fit(X_train, y_train[: , i], alpha=0.00001, iterations=100, lambda_ = 1)\n",
    "            \n",
    "            p = model.predict(X_train, decision_boundary)\n",
    "            print('Train Accuracy: %f'%(np.mean(p == y_train[:, i]) * 100))\n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        return self.softmax(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5ac94605",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = MultipleLogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1fbc30fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training model 1 with alpha= 3e-05\n",
      "Iteration    0: Cost     1.61   \n",
      "Iteration   10: Cost     1.30   \n",
      "Iteration   20: Cost     1.00   \n",
      "Iteration   30: Cost     0.71   \n",
      "Iteration   40: Cost     0.44   \n",
      "Iteration   50: Cost     0.24   \n",
      "Iteration   60: Cost     0.13   \n",
      "Iteration   70: Cost     0.09   \n",
      "Iteration   80: Cost     0.08   \n",
      "Iteration   90: Cost     0.08   \n",
      "Iteration   99: Cost     0.08   \n",
      "Train Accuracy: 99.187500\n",
      "training model 1 with alpha= 1e-05\n",
      "Iteration    0: Cost     0.08   \n",
      "Iteration   10: Cost     0.08   \n",
      "Iteration   20: Cost     0.09   \n",
      "Iteration   30: Cost     0.09   \n",
      "Iteration   40: Cost     0.09   \n",
      "Iteration   50: Cost     0.09   \n",
      "Iteration   60: Cost     0.09   \n",
      "Iteration   70: Cost     0.10   \n",
      "Iteration   80: Cost     0.10   \n",
      "Iteration   90: Cost     0.10   \n",
      "Iteration   99: Cost     0.10   \n",
      "Train Accuracy: 99.187500\n",
      "training model 2 with alpha= 3e-05\n",
      "Iteration    0: Cost     2.62   \n",
      "Iteration   10: Cost     2.37   \n",
      "Iteration   20: Cost     2.13   \n",
      "Iteration   30: Cost     1.88   \n",
      "Iteration   40: Cost     1.63   \n",
      "Iteration   50: Cost     1.38   \n",
      "Iteration   60: Cost     1.14   \n",
      "Iteration   70: Cost     0.92   \n",
      "Iteration   80: Cost     0.71   \n",
      "Iteration   90: Cost     0.53   \n",
      "Iteration   99: Cost     0.40   \n",
      "Train Accuracy: 77.250000\n",
      "training model 2 with alpha= 1e-05\n",
      "Iteration    0: Cost     0.39   \n",
      "Iteration   10: Cost     0.35   \n",
      "Iteration   20: Cost     0.32   \n",
      "Iteration   30: Cost     0.29   \n",
      "Iteration   40: Cost     0.26   \n",
      "Iteration   50: Cost     0.23   \n",
      "Iteration   60: Cost     0.21   \n",
      "Iteration   70: Cost     0.19   \n",
      "Iteration   80: Cost     0.18   \n",
      "Iteration   90: Cost     0.16   \n",
      "Iteration   99: Cost     0.15   \n",
      "Train Accuracy: 94.562500\n",
      "training model 3 with alpha= 3e-05\n",
      "Iteration    0: Cost     2.18   \n",
      "Iteration   10: Cost     1.85   \n",
      "Iteration   20: Cost     1.52   \n",
      "Iteration   30: Cost     1.20   \n",
      "Iteration   40: Cost     0.88   \n",
      "Iteration   50: Cost     0.59   \n",
      "Iteration   60: Cost     0.37   \n",
      "Iteration   70: Cost     0.23   \n",
      "Iteration   80: Cost     0.15   \n",
      "Iteration   90: Cost     0.13   \n",
      "Iteration   99: Cost     0.12   \n",
      "Train Accuracy: 98.562500\n",
      "training model 3 with alpha= 1e-05\n",
      "Iteration    0: Cost     0.12   \n",
      "Iteration   10: Cost     0.12   \n",
      "Iteration   20: Cost     0.12   \n",
      "Iteration   30: Cost     0.12   \n",
      "Iteration   40: Cost     0.13   \n",
      "Iteration   50: Cost     0.13   \n",
      "Iteration   60: Cost     0.13   \n",
      "Iteration   70: Cost     0.14   \n",
      "Iteration   80: Cost     0.14   \n",
      "Iteration   90: Cost     0.15   \n",
      "Iteration   99: Cost     0.15   \n",
      "Train Accuracy: 98.812500\n",
      "training model 4 with alpha= 3e-05\n",
      "Iteration    0: Cost     1.48   \n",
      "Iteration   10: Cost     1.31   \n",
      "Iteration   20: Cost     1.13   \n",
      "Iteration   30: Cost     0.95   \n",
      "Iteration   40: Cost     0.78   \n",
      "Iteration   50: Cost     0.61   \n",
      "Iteration   60: Cost     0.45   \n",
      "Iteration   70: Cost     0.32   \n",
      "Iteration   80: Cost     0.22   \n",
      "Iteration   90: Cost     0.15   \n",
      "Iteration   99: Cost     0.12   \n",
      "Train Accuracy: 96.812500\n",
      "training model 4 with alpha= 1e-05\n",
      "Iteration    0: Cost     0.12   \n",
      "Iteration   10: Cost     0.11   \n",
      "Iteration   20: Cost     0.10   \n",
      "Iteration   30: Cost     0.09   \n",
      "Iteration   40: Cost     0.09   \n",
      "Iteration   50: Cost     0.09   \n",
      "Iteration   60: Cost     0.08   \n",
      "Iteration   70: Cost     0.08   \n",
      "Iteration   80: Cost     0.08   \n",
      "Iteration   90: Cost     0.08   \n",
      "Iteration   99: Cost     0.08   \n",
      "Train Accuracy: 99.125000\n"
     ]
    }
   ],
   "source": [
    "models.fit(X_train, y_train, lambda_=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6edb31c9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost     0.15   \n",
      "Iteration   10: Cost     0.14   \n",
      "Iteration   20: Cost     0.13   \n",
      "Iteration   30: Cost     0.12   \n",
      "Iteration   40: Cost     0.11   \n",
      "Iteration   50: Cost     0.11   \n",
      "Iteration   60: Cost     0.10   \n",
      "Iteration   70: Cost     0.10   \n",
      "Iteration   80: Cost     0.09   \n",
      "Iteration   90: Cost     0.09   \n",
      "Iteration   99: Cost     0.09   \n"
     ]
    }
   ],
   "source": [
    "models.models[1].fit(X_train, y_train[:, 1], alpha=0.00001, iterations=100, lambda_ = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7ab2454e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.array(models.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1025ec9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 2, ..., 0, 1, 1])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02213c9",
   "metadata": {},
   "source": [
    "### Get true labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "bbbfbb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = []\n",
    "for row in y_train:\n",
    "    answer.append(np.argmax(row))\n",
    "answer = np.array(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "eaa9215f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 98.187500\n"
     ]
    }
   ],
   "source": [
    "#Compute accuracy on our training set\n",
    "print('Train Accuracy: %f'%(np.mean(pred == answer) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5295296f",
   "metadata": {},
   "source": [
    "### Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f314c411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://miro.medium.com/max/1400/1*eS4zV29dd9vFo0KQSk8s7w.png\" width=\"500\" height=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://miro.medium.com/max/1400/1*eS4zV29dd9vFo0KQSk8s7w.png\", width=500, height=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e9b8554a",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_matrix, precision = models.score_matrix(pred, answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "cf362880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[318.,   2.,   2.,   0.],\n",
       "       [  4., 525.,   4.,   3.],\n",
       "       [  5.,   1., 435.,   5.],\n",
       "       [  0.,   1.,   2., 293.]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a2c6b723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9875776397515528,\n",
       " 0.9794776119402985,\n",
       " 0.9753363228699552,\n",
       " 0.9898648648648649]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "186756dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global precision =  0.9830641098566678\n"
     ]
    }
   ],
   "source": [
    "print(\"global precision = \", sum(precision)/len(precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377837d6",
   "metadata": {},
   "source": [
    "## Trying prediction on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5298c8",
   "metadata": {},
   "source": [
    "Reconvert Array of Integer (category number) to 4 arrays of 0and1 for each House"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0d472e1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 0, 1, 1, 3, 2, 1, 2, 1, 1, 3, 3, 3, 3, 1, 2, 3, 2, 1, 1, 3,\n",
       "       2, 2, 1, 1, 2, 3, 0, 2, 1, 1, 0, 1, 1, 2, 0, 2, 3, 2, 2, 3, 1, 3,\n",
       "       1, 2, 3, 0, 2, 3, 2, 1, 2, 2, 1, 3, 3, 2, 1, 0, 0, 0, 1, 1, 2, 1,\n",
       "       1, 2, 1, 2, 3, 3, 0, 2, 0, 1, 1, 0, 2, 0, 2, 1, 3, 0, 0, 1, 1, 1,\n",
       "       2, 2, 1, 3, 0, 1, 2, 2, 0, 3, 1, 2, 1, 1, 1, 3, 2, 2, 1, 3, 3, 1,\n",
       "       1, 3, 0, 2, 1, 2, 2, 1, 3, 2, 0, 1, 0, 0, 2, 2, 3, 2, 1, 2, 2, 0,\n",
       "       0, 3, 1, 2, 2, 3, 0, 1, 1, 2, 0, 2, 0, 2, 2, 1, 1, 2, 1, 0, 0, 2,\n",
       "       2, 0, 1, 3, 1, 2, 2, 1, 1, 1, 2, 3, 1, 1, 1, 1, 0, 1, 1, 3, 1, 0,\n",
       "       0, 1, 1, 3, 2, 2, 0, 3, 0, 0, 1, 2, 1, 3, 0, 1, 0, 2, 2, 1, 2, 1,\n",
       "       1, 0, 0, 1, 2, 1, 0, 3, 2, 1, 0, 2, 0, 1, 3, 2, 3, 3, 1, 1, 0, 0,\n",
       "       0, 0, 1, 1, 0, 2, 2, 2, 1, 1, 0, 1, 3, 2, 1, 1, 1, 3, 0, 0, 1, 0,\n",
       "       1, 1, 0, 0, 2, 2, 1, 1, 3, 3, 1, 1, 2, 2, 1, 2, 1, 2, 3, 2, 2, 1,\n",
       "       1, 2, 2, 0, 2, 2, 1, 3, 2, 0, 2, 1, 2, 0, 0, 1, 1, 1, 1, 2, 1, 2,\n",
       "       2, 3, 0, 2, 0, 0, 2, 1, 1, 0, 3, 1, 1, 1, 1, 1, 1, 1, 2, 3, 1, 2,\n",
       "       1, 3, 1, 1, 1, 0, 2, 1, 2, 3, 1, 3, 3, 2, 2, 3, 2, 2, 3, 1, 3, 0,\n",
       "       2, 1, 0, 2, 2, 1, 2, 1, 0, 1, 1, 3, 2, 0, 1, 1, 1, 2, 2, 3, 3, 0,\n",
       "       1, 2, 3, 1, 1, 1, 0, 1, 2, 0, 0, 2, 1, 0, 1, 1, 0, 1, 3, 0, 0, 2,\n",
       "       1, 1, 2, 3, 0, 3, 0, 1, 0, 0, 0, 0, 2, 2, 1, 2, 3, 1, 1, 2, 2, 3,\n",
       "       1, 1, 2, 2])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = np.array(test)\n",
    "test_pred = models.softmax(X_test)\n",
    "test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "bdbd6f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = np.zeros((len(test_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "31c934ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(test_pred, columns=['Hogwarts House'])\n",
    "target_columns = ['Gryffindor', 'Hufflepuff', 'Ravenclaw', 'Slytherin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "1a55c4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df)):\n",
    "    idx = df['Hogwarts House'][i]\n",
    "    df['Hogwarts House'][i] = target_columns[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "426f393c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Index'] = df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f3ce2ab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hogwarts House</th>\n",
       "      <th>Index</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hufflepuff</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ravenclaw</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gryffindor</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hufflepuff</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hufflepuff</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>Slytherin</td>\n",
       "      <td>395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>Hufflepuff</td>\n",
       "      <td>396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>Hufflepuff</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>Ravenclaw</td>\n",
       "      <td>398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>Ravenclaw</td>\n",
       "      <td>399</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Hogwarts House  Index\n",
       "Index                      \n",
       "0         Hufflepuff      0\n",
       "1          Ravenclaw      1\n",
       "2         Gryffindor      2\n",
       "3         Hufflepuff      3\n",
       "4         Hufflepuff      4\n",
       "...              ...    ...\n",
       "395        Slytherin    395\n",
       "396       Hufflepuff    396\n",
       "397       Hufflepuff    397\n",
       "398        Ravenclaw    398\n",
       "399        Ravenclaw    399\n",
       "\n",
       "[400 rows x 2 columns]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfea2fe",
   "metadata": {},
   "source": [
    "### Save to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "096a10c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('prediction.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728e50c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
