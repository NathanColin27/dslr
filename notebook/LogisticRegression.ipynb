{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e6cef907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import Modules/Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "import warnings\n",
    "#suppress warnings with numpy for sigmoid function\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2f2ba7",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "df612c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train = pd.read_csv('../datasets/clean/cleaned_train.csv', index_col = False)\n",
    "test = pd.read_csv('../datasets/clean/cleaned_test.csv', index_col = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d55aac2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Astronomy</th>\n",
       "      <th>Herbology</th>\n",
       "      <th>Divination</th>\n",
       "      <th>Muggle Studies</th>\n",
       "      <th>Ancient Runes</th>\n",
       "      <th>History of Magic</th>\n",
       "      <th>Transfiguration</th>\n",
       "      <th>Potions</th>\n",
       "      <th>Charms</th>\n",
       "      <th>Flying</th>\n",
       "      <th>Gryffindor</th>\n",
       "      <th>Hufflepuff</th>\n",
       "      <th>Ravenclaw</th>\n",
       "      <th>Slytherin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.014194</td>\n",
       "      <td>0.878628</td>\n",
       "      <td>0.377371</td>\n",
       "      <td>1.021139</td>\n",
       "      <td>0.345639</td>\n",
       "      <td>0.512444</td>\n",
       "      <td>0.219633</td>\n",
       "      <td>-0.686183</td>\n",
       "      <td>1.204553</td>\n",
       "      <td>-0.500330</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.137535</td>\n",
       "      <td>-1.365690</td>\n",
       "      <td>-2.109573</td>\n",
       "      <td>-0.540256</td>\n",
       "      <td>-1.204191</td>\n",
       "      <td>0.258503</td>\n",
       "      <td>0.653769</td>\n",
       "      <td>0.412462</td>\n",
       "      <td>-1.002983</td>\n",
       "      <td>-1.386928</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.780078</td>\n",
       "      <td>1.261379</td>\n",
       "      <td>0.718622</td>\n",
       "      <td>1.828915</td>\n",
       "      <td>1.005195</td>\n",
       "      <td>0.133871</td>\n",
       "      <td>1.314249</td>\n",
       "      <td>0.882556</td>\n",
       "      <td>1.825184</td>\n",
       "      <td>0.086673</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Astronomy  Herbology  Divination  Muggle Studies  Ancient Runes  \\\n",
       "0  -1.014194   0.878628    0.377371        1.021139       0.345639   \n",
       "1  -1.137535  -1.365690   -2.109573       -0.540256      -1.204191   \n",
       "2  -0.780078   1.261379    0.718622        1.828915       1.005195   \n",
       "\n",
       "   History of Magic  Transfiguration   Potions    Charms    Flying  \\\n",
       "0          0.512444         0.219633 -0.686183  1.204553 -0.500330   \n",
       "1          0.258503         0.653769  0.412462 -1.002983 -1.386928   \n",
       "2          0.133871         1.314249  0.882556  1.825184  0.086673   \n",
       "\n",
       "   Gryffindor  Hufflepuff  Ravenclaw  Slytherin  \n",
       "0           0           0          1          0  \n",
       "1           0           0          0          1  \n",
       "2           0           0          1          0  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c7696fc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Astronomy</th>\n",
       "      <th>Herbology</th>\n",
       "      <th>Divination</th>\n",
       "      <th>Muggle Studies</th>\n",
       "      <th>Ancient Runes</th>\n",
       "      <th>History of Magic</th>\n",
       "      <th>Transfiguration</th>\n",
       "      <th>Potions</th>\n",
       "      <th>Charms</th>\n",
       "      <th>Flying</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.261390</td>\n",
       "      <td>0.360013</td>\n",
       "      <td>1.165280</td>\n",
       "      <td>-0.289017</td>\n",
       "      <td>-0.965420</td>\n",
       "      <td>0.281215</td>\n",
       "      <td>0.377241</td>\n",
       "      <td>-0.724744</td>\n",
       "      <td>-0.126065</td>\n",
       "      <td>-0.364411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.789243</td>\n",
       "      <td>0.349486</td>\n",
       "      <td>0.768919</td>\n",
       "      <td>1.536298</td>\n",
       "      <td>1.007714</td>\n",
       "      <td>0.790127</td>\n",
       "      <td>0.406955</td>\n",
       "      <td>0.814602</td>\n",
       "      <td>1.375558</td>\n",
       "      <td>-0.493877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.539127</td>\n",
       "      <td>-1.403671</td>\n",
       "      <td>0.352583</td>\n",
       "      <td>-0.833736</td>\n",
       "      <td>0.868643</td>\n",
       "      <td>-1.927101</td>\n",
       "      <td>-2.122591</td>\n",
       "      <td>-1.302330</td>\n",
       "      <td>-1.095106</td>\n",
       "      <td>1.825147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Astronomy  Herbology  Divination  Muggle Studies  Ancient Runes  \\\n",
       "0   1.261390   0.360013    1.165280       -0.289017      -0.965420   \n",
       "1  -0.789243   0.349486    0.768919        1.536298       1.007714   \n",
       "2   0.539127  -1.403671    0.352583       -0.833736       0.868643   \n",
       "\n",
       "   History of Magic  Transfiguration   Potions    Charms    Flying  \n",
       "0          0.281215         0.377241 -0.724744 -0.126065 -0.364411  \n",
       "1          0.790127         0.406955  0.814602  1.375558 -0.493877  \n",
       "2         -1.927101        -2.122591 -1.302330 -1.095106  1.825147  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d6890b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split X (independent variables) with the target value Y\n",
    "target_columns = ['Gryffindor', 'Hufflepuff', 'Ravenclaw', 'Slytherin']\n",
    "X_train = np.array(train.drop(columns = target_columns))\n",
    "y_train = np.array(train[target_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "14b813db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X_train is: (1600, 10)\n",
      "The shape of y_train is: (1600, 4)\n",
      "We have m = 1600 training examples\n"
     ]
    }
   ],
   "source": [
    "print ('The shape of X_train is: ' + str(X_train.shape))\n",
    "print ('The shape of y_train is: ' + str(y_train.shape))\n",
    "print ('We have m = %d training examples' % (len(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a5eee7bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.01419368,  0.87862839,  0.37737107,  1.02113913,  0.34563861,\n",
       "        0.51244445,  0.21963306, -0.68618295,  1.20455291, -0.50032993])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "35fa8110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 0, 0])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To Access the ith column \n",
    "y_train[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c91377",
   "metadata": {},
   "source": [
    "### Sigmoid function\n",
    "\n",
    "For logistic regression, the model is represented as\n",
    "\n",
    "$$ f_{\\mathbf{w},b}(x) = g(\\mathbf{w}\\cdot \\mathbf{x} + b)$$\n",
    "where function $g$ is the sigmoid function. The sigmoid function is defined as:\n",
    "\n",
    "$$g(z) = \\frac{1}{1+e^{-z}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8684fd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "    Args:\n",
    "        z (ndarray): A scalar, numpy array of any size.\n",
    "    Returns:\n",
    "        g (ndarray): sigmoid(z), with the same shape as z\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b33279a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid(0) = 0.5\n"
     ]
    }
   ],
   "source": [
    "print (\"sigmoid(0) = \" + str(sigmoid(0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0013e206",
   "metadata": {},
   "source": [
    "### Cost function for logistic regression\n",
    "\n",
    "\n",
    "Recall that for logistic regression, the cost function is of the form \n",
    "\n",
    "$$ J(\\mathbf{w},b) = \\frac{1}{m}\\sum_{i=0}^{m-1} \\left[ loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) \\right] \\tag{1}$$\n",
    "\n",
    "where\n",
    "* m is the number of training examples in the dataset\n",
    "\n",
    "\n",
    "* $loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)})$ is the cost for a single data point, which is - \n",
    "\n",
    "    $$loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = (-y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\tag{2}$$\n",
    "    \n",
    "    \n",
    "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ is the model's prediction, while $y^{(i)}$, which is the actual label\n",
    "\n",
    "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(\\mathbf{w} \\cdot \\mathbf{x^{(i)}} + b)$ where function $g$ is the sigmoid function.\n",
    "    * It might be helpful to first calculate an intermediate variable $z_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x^{(i)}} + b = w_0x^{(i)}_0 + ... + w_{n-1}x^{(i)}_{n-1} + b$ where $n$ is the number of features, before calculating $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(z_{\\mathbf{w},b}(\\mathbf{x}^{(i)}))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "0cfc6cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, w, b, lambda_= 1):\n",
    "    \"\"\"\n",
    "    Computes the cost over all examples\n",
    "    Args:\n",
    "      X : (ndarray Shape (m,n)) data, m examples by n features\n",
    "      y : (array_like Shape (m,)) target value \n",
    "      w : (array_like Shape (n,)) Values of parameters of the model      \n",
    "      b : scalar Values of bias parameter of the model\n",
    "      lambda_: unused placeholder\n",
    "    Returns:\n",
    "      total_cost: (scalar)         cost \n",
    "    \"\"\"\n",
    "\n",
    "    m, n = X.shape\n",
    "    \n",
    "    cost = 0\n",
    "    for i in range(m):\n",
    "        z = np.dot(X[i],w) + b\n",
    "        f_wb = sigmoid(z)\n",
    "        cost += -y[i]*np.log(f_wb) - (1-y[i])*np.log(1-f_wb)\n",
    "    total_cost = cost/m\n",
    "    \n",
    "\n",
    "    return total_costz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "19649090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at initial w (zeros): 0.693\n"
     ]
    }
   ],
   "source": [
    "m, n = X_train.shape\n",
    "\n",
    "# Compute and display cost with w initialized to zeroes\n",
    "initial_w = np.zeros(n)\n",
    "initial_b = 0.\n",
    "cost = compute_cost(X_train, y_train[:, 0], initial_w, initial_b)\n",
    "print('Cost at initial w (zeros): {:.3f}'.format(cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4c4d1e",
   "metadata": {},
   "source": [
    "###  Gradient for logistic regression\n",
    "\n",
    "In this section, you will implement the gradient for logistic regression.\n",
    "\n",
    "Recall that the gradient descent algorithm is:\n",
    "\n",
    "$$\\begin{align*}& \\text{repeat until convergence:} \\; \\lbrace \\newline \\; & b := b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b} \\newline       \\; & w_j := w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{1}  \\; & \\text{for j := 0..n-1}\\newline & \\rbrace\\end{align*}$$\n",
    "\n",
    "where, parameters $b$, $w_j$ are all updated simultaniously\n",
    "\n",
    "compute $\\frac{\\partial J(\\mathbf{w},b)}{\\partial w}$, $\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}$ from equations (2) and (3) below.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - \\mathbf{y}^{(i)}) \\tag{2}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - \\mathbf{y}^{(i)})x_{j}^{(i)} \\tag{3}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "821a0605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, w, b, lambda_=1): \n",
    "    \"\"\"\n",
    "    Computes the gradient for logistic regression \n",
    " \n",
    "    Args:\n",
    "      X : (ndarray Shape (m,n)) variable such as house size \n",
    "      y : (array_like Shape (m,1)) actual value \n",
    "      w : (array_like Shape (n,1)) values of parameters of the model      \n",
    "      b : (scalar)                 value of parameter of the model \n",
    "      lambda_: unused placeholder.\n",
    "    Returns\n",
    "      dj_dw: (array_like Shape (n,1)) The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db: (scalar)                The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    dj_dw = np.zeros(w.shape)\n",
    "    dj_db = 0.\n",
    "\n",
    "    z = np.dot(X, w) + b\n",
    "    f_wb = sigmoid(z)\n",
    "\n",
    "    for j in range(n):\n",
    "        dj_dw[j] = (np.sum(np.dot(f_wb - y, X.T[j])))\n",
    "\n",
    "    dj_dw = dj_dw / m\n",
    "    dj_db = np.sum(f_wb - y) / m\n",
    "\n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d194133",
   "metadata": {},
   "source": [
    "### Learning parameters using gradient descent \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "22f0b27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters, lambda_=1): \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn theta. Updates theta by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      X :    (array_like Shape (m, n)\n",
    "      y :    (array_like Shape (m,))\n",
    "      w_in : (array_like Shape (n,))  Initial values of parameters of the model\n",
    "      b_in : (scalar)                 Initial value of parameter of the model\n",
    "      cost_function:                  function to compute cost\n",
    "      alpha : (float)                 Learning rate\n",
    "      num_iters : (int)               number of iterations to run gradient descent\n",
    "      lambda_ (scalar, float)         regularization constant\n",
    "      \n",
    "    Returns:\n",
    "      w : (array_like Shape (n,)) Updated values of parameters of the model after\n",
    "          running gradient descent\n",
    "      b : (scalar)                Updated value of parameter of the model after\n",
    "          running gradient descent\n",
    "    \"\"\"\n",
    "    \n",
    "    # number of training examples\n",
    "    m = len(X)\n",
    "    \n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    w_history = []\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "\n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_db, dj_dw = gradient_function(X, y, w_in, b_in, lambda_)   \n",
    "\n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        w_in = w_in - alpha * dj_dw\n",
    "        b_in = b_in - alpha * dj_db\n",
    "       \n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            cost =  cost_function(X, y, w_in, b_in, lambda_)\n",
    "            J_history.append(cost)\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters/10) == 0 or i == (num_iters-1):\n",
    "            w_history.append(w_in)\n",
    "            print(f\"Iteration {i:4}: Cost {float(J_history[-1]):8.2f}   \")\n",
    "        \n",
    "    return w_in, b_in, J_history, w_history #return w and J,w history for graphing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033d2dab",
   "metadata": {},
   "source": [
    "## Fit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "d46a2882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost  2584.83   \n",
      "Iteration 1000: Cost      nan   \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [175], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m iterations \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000\u001b[39m\n\u001b[1;32m      8\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0003\u001b[39m\n\u001b[0;32m---> 10\u001b[0m w,b, J_history,_ \u001b[38;5;241m=\u001b[39m gradient_descent(X_train ,y_train[:, \u001b[38;5;241m0\u001b[39m], initial_w, initial_b, \n\u001b[1;32m     11\u001b[0m                                    compute_cost, compute_gradient, alpha, iterations, \u001b[38;5;241m0.5\u001b[39m)\n",
      "Cell \u001b[0;32mIn [174], line 41\u001b[0m, in \u001b[0;36mgradient_descent\u001b[0;34m(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters, lambda_)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Save cost J at each iteration\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m100000\u001b[39m:      \u001b[38;5;66;03m# prevent resource exhaustion \u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     cost \u001b[38;5;241m=\u001b[39m  \u001b[43mcost_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     J_history\u001b[38;5;241m.\u001b[39mappend(cost)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Print cost every at intervals 10 times or as many iterations if < 10\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [167], line 22\u001b[0m, in \u001b[0;36mcompute_cost\u001b[0;34m(X, y, w, b, lambda_)\u001b[0m\n\u001b[1;32m     18\u001b[0m     pred \u001b[38;5;241m=\u001b[39m sigmoid(z)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#     pred[pred == 1] = 1-1e-9 #hard cap max threshold\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m     total_cost \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m m) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msum(\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpred\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     23\u001b[0m     reg_cost \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(np\u001b[38;5;241m.\u001b[39mdot(w, w\u001b[38;5;241m.\u001b[39mT))\n\u001b[1;32m     24\u001b[0m     total_cost \u001b[38;5;241m=\u001b[39m total_cost \u001b[38;5;241m+\u001b[39m (lambda_\u001b[38;5;241m/\u001b[39m(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m m)) \u001b[38;5;241m*\u001b[39m reg_cost\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "m, n = X_train.shape\n",
    "initial_w = 0.01 * (np.random.rand(n).reshape(-1,1) - 0.5)\n",
    "initial_b = -8\n",
    "\n",
    "\n",
    "# Some gradient descent settings\n",
    "iterations = 10000\n",
    "alpha = 0.0003\n",
    "\n",
    "w,b, J_history,_ = gradient_descent(X_train ,y_train[:, 0], initial_w, initial_b, \n",
    "                                   compute_cost, compute_gradient, alpha, iterations, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "b737f28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost  2596.62   \n",
      "Iteration  100: Cost  2594.64   \n",
      "Iteration  200: Cost  2592.66   \n",
      "Iteration  300: Cost  2590.69   \n",
      "Iteration  400: Cost  2588.73   \n",
      "Iteration  500: Cost  2586.77   \n",
      "Iteration  600: Cost  2584.81   \n",
      "Iteration  700: Cost  2582.86   \n",
      "Iteration  800: Cost  2580.91   \n",
      "Iteration  900: Cost  2578.98   \n",
      "Iteration  999: Cost  2577.06   \n"
     ]
    }
   ],
   "source": [
    "iterations = 10000\n",
    "alpha = 0.0001\n",
    "\n",
    "w,b, J_history,_ = gradient_descent(X_train ,y_train[:, 0], w, b, \n",
    "                                   compute_cost, compute_gradient, alpha, iterations, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8da5a11",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "27707f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, w, b): \n",
    "    \"\"\"\n",
    "    Predict whether the label is 0 or 1 using learned logistic\n",
    "    regression parameters w\n",
    "    \n",
    "    Args:\n",
    "    X : (ndarray Shape (m, n))\n",
    "    w : (array_like Shape (n,))      Parameters of the model\n",
    "    b : (scalar, float)              Parameter of the model\n",
    "\n",
    "    Returns:\n",
    "    p: (ndarray (m,1))\n",
    "        The predictions for X using a threshold at 0.5\n",
    "    \"\"\"\n",
    "    # number of training examples\n",
    "    m, n = X.shape\n",
    "    p = np.zeros(m)\n",
    "\n",
    "    f_wb = sigmoid(np.dot(X, w) + b)\n",
    "    for i, prob in enumerate(f_wb):\n",
    "        p[i] = 1 if prob >= 0.50 else 0\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d276ae8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 61.062500\n"
     ]
    }
   ],
   "source": [
    "#Compute accuracy on our training set\n",
    "p = predict(X_train, w,b)\n",
    "print('Train Accuracy: %f'%(np.mean(p == y_train[:, 3]) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710259b0",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b1a58e",
   "metadata": {},
   "source": [
    "# Make a class out of it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "8e3c1c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \"\"\"\n",
    "        A class to perform Logistic Regression\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "            Define attributes which will be passed later\n",
    "        \"\"\"\n",
    "        # train data and label\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        \n",
    "        # m: number of observations\n",
    "        self.m = None\n",
    "        \n",
    "        # n: number of independent variables (X)\n",
    "        self.n = None\n",
    "        \n",
    "        # store historic value of cost function.\n",
    "        self.costs = []\n",
    "\n",
    "        self.J_history = []\n",
    "        self.w_history = []\n",
    "        \n",
    "        # parameteres (weights)\n",
    "        self.thetas = None\n",
    "        \n",
    "        # regularization constant lambda_ (scalar, float)\n",
    "        self.lambda_ = None\n",
    "\n",
    "    \n",
    "    def initialize(self, X):\n",
    "        \"\"\"\n",
    "        Initialize thetas (weights) where 1st entry is the b (intercept) and the rest w (therefore size n+1)\n",
    "        X is concatenated with an np.ones of shape (m, 1) for the b (intercept)\n",
    "        \"\"\"\n",
    "        self.thetas = np.zeros((self.n + 1, 1))\n",
    "\n",
    "        \n",
    "    def save_weights(self):\n",
    "        return {'w': self.thetas[1:].tolist(), 'b': self.thetas[0]}\n",
    "    \n",
    "    def load_weights(self, weights):\n",
    "        self.thetas = weights\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"\n",
    "        Compute the sigmoid of z\n",
    "        Args:\n",
    "            z (ndarray): A scalar, numpy array of any size.\n",
    "        Returns:\n",
    "            g (ndarray): sigmoid(z), with the same shape as z\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    \n",
    "    def compute_cost(self, X, y, thetas):\n",
    "        \"\"\"\n",
    "        Computes the cost over all examples\n",
    "        Args:\n",
    "          X : (ndarray Shape (m,n)) data, m examples by n features\n",
    "          y : (array_like Shape (m,)) target value \n",
    "          w : (array_like Shape (n,)) Values of parameters of the model      \n",
    "          b : scalar Values of bias parameter of the model\n",
    "          lambda_: unused placeholder\n",
    "        Returns:\n",
    "          total_cost: (scalar)         cost \n",
    "        \"\"\"\n",
    "\n",
    "        m, n = X.shape\n",
    "\n",
    "        z = np.dot(X, thetas)\n",
    "        pred = self.sigmoid(z)\n",
    "\n",
    "#         pred[pred == 1] = 1-1e-9 #hard cap max threshold\n",
    "        \n",
    "        \n",
    "        cost = np.dot(-y.T, np.log(pred)) - (np.dot( (1-y).T, np.log(1 - pred)))\n",
    "        total_cost = cost / m\n",
    "    \n",
    "        reg_cost = np.sum(np.dot(w, w.T))\n",
    "        total_cost = total_cost + (self.lambda_/(2 * m)) * reg_cost\n",
    "        \n",
    "        return total_cost\n",
    "\n",
    "    \n",
    "    def compute_gradient(self, X, y, thetas, lambda_): \n",
    "        \"\"\"\n",
    "        Computes the gradient for logistic regression \n",
    "\n",
    "        Args:\n",
    "          X : (ndarray Shape (m,n)) variable such as house size \n",
    "          y : (array_like Shape (m,1)) actual value \n",
    "          w : (array_like Shape (n,1)) values of parameters of the model      \n",
    "          b : (scalar)                 value of parameter of the model \n",
    "          lambda_: unused placeholder.\n",
    "        Returns\n",
    "          dj_dw: (array_like Shape (n,1)) The gradient of the cost w.r.t. the parameters w. \n",
    "          dj_db: (scalar)                The gradient of the cost w.r.t. the parameter b. \n",
    "        \"\"\"\n",
    "        m, n = X.shape\n",
    "        dj = np.zeros(thetas.shape)\n",
    "\n",
    "        z = np.dot(X, thetas)\n",
    "        f_wb = self.sigmoid(z)\n",
    "\n",
    "        dj = np.dot(X.T, f_wb - np.reshape(y,(len(y),1)) )\n",
    "        \n",
    "        return dj\n",
    "\n",
    "\n",
    "    def gradient_descent(self, cost_function, gradient_function, alpha, num_iters, show_every): \n",
    "        \"\"\"\n",
    "        Performs batch gradient descent to learn theta. Updates theta by taking \n",
    "        num_iters gradient steps with learning rate alpha\n",
    "\n",
    "        Args:\n",
    "          X :    (array_like Shape (m, n)\n",
    "          y :    (array_like Shape (m,))\n",
    "          w_in : (array_like Shape (n,))  Initial values of parameters of the model\n",
    "          b_in : (scalar)                 Initial value of parameter of the model\n",
    "          cost_function:                  function to compute cost\n",
    "          alpha : (float)                 Learning rate\n",
    "          num_iters : (int)               number of iterations to run gradient descent\n",
    "          lambda_ (scalar, float)         regularization constant\n",
    "\n",
    "        Returns:\n",
    "          w : (array_like Shape (n,)) Updated values of parameters of the model after\n",
    "              running gradient descent\n",
    "          b : (scalar)                Updated value of parameter of the model after\n",
    "              running gradient descent\n",
    "        \"\"\"\n",
    "        \n",
    "        for i in range(num_iters):\n",
    "\n",
    "            # Calculate the gradient and update the parameters\n",
    "            dj = gradient_function(self.X, self.y, self.thetas, self.lambda_)   \n",
    "\n",
    "            # Update Parameters using w, b, alpha and gradient\n",
    "            self.thetas = self.thetas - alpha * dj\n",
    "\n",
    "            # Save cost J at each iteration\n",
    "            if i<100000:      # prevent resource exhaustion \n",
    "                cost =  cost_function(self.X, self.y, self.thetas)\n",
    "\n",
    "            # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "            if i % show_every == 0 or i == num_iters-1:\n",
    "                self.J_history.append(cost)\n",
    "#                 self.w_history.append(self.w)\n",
    "                print(f\"Iteration {i:4}: Cost {float(self.J_history[-1]):8.2f}   \")\n",
    "        \n",
    "    \n",
    "    def fit(self, X, y, alpha=0.001, iterations=1500, show_every=None, lambda_= 1):\n",
    "        \"\"\"\n",
    "        setup attributes and apply training\n",
    "        \"\"\"\n",
    "        \n",
    "        # m: number of observations\n",
    "        self.m, self.n = X.shape\n",
    "        \n",
    "        # train data and label\n",
    "        self.X = np.c_[np.ones((self.m, 1)), X]\n",
    "        self.y = y\n",
    "\n",
    "        # regularization coefficient\n",
    "        self.lambda_ = lambda_\n",
    "        \n",
    "        # init weights if first call\n",
    "        if type(self.thetas) is not np.ndarray:\n",
    "            self.initialize(X)\n",
    "        if show_every == None:\n",
    "            if (iterations <= 10):\n",
    "                show_every = 1\n",
    "            else:\n",
    "                show_every = iterations // 10 \n",
    "            \n",
    "        # Perform Gradient Descent\n",
    "        self.gradient_descent(self.compute_cost, self.compute_gradient, alpha, iterations, show_every)\n",
    "\n",
    "    \n",
    "    def predict(self, X, decision_boundary = None): \n",
    "        \"\"\"\n",
    "        Predict whether the label is 0 or 1 using learned logistic\n",
    "        regression parameters w\n",
    "\n",
    "        Args:\n",
    "        X : (ndarray Shape (m, n))\n",
    "        w : (array_like Shape (n,))      Parameters of the model\n",
    "        b : (scalar, float)              Parameter of the model\n",
    "\n",
    "        Returns:\n",
    "        p: (ndarray (m,1))\n",
    "            The predictions for X using a threshold at 0.5\n",
    "        \"\"\"\n",
    "        \n",
    "        # Check if number of features matches our model\n",
    "        \n",
    "        \n",
    "        # number of training examples\n",
    "        m, n = X.shape\n",
    "        p = np.zeros(m)\n",
    "\n",
    "        X = np.c_[np.ones((self.m, 1)), X]\n",
    "        f_wb = self.sigmoid(np.dot(X, self.thetas))\n",
    "        \n",
    "        if decision_boundary != None:\n",
    "            for i, prob in enumerate(f_wb):\n",
    "                p[i] = 1 if prob >= decision_boundary else 0\n",
    "            return p\n",
    "        \n",
    "        return f_wb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "cb2c0bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "88035472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost     0.69   \n",
      "Iteration 1000: Cost     0.10   \n",
      "Iteration 2000: Cost     0.07   \n",
      "Iteration 3000: Cost     0.07   \n",
      "Iteration 4000: Cost     0.07   \n",
      "Iteration 5000: Cost     0.06   \n",
      "Iteration 6000: Cost     0.06   \n",
      "Iteration 7000: Cost     0.06   \n",
      "Iteration 8000: Cost     0.06   \n",
      "Iteration 9000: Cost     0.06   \n",
      "Iteration 9999: Cost     0.06   \n"
     ]
    }
   ],
   "source": [
    "lr.fit(X_train, y_train[: , 1], alpha=0.00001, iterations=10000, lambda_ = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "47f306b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost     0.10   \n",
      "Iteration   10: Cost     0.09   \n",
      "Iteration   20: Cost     0.09   \n",
      "Iteration   30: Cost     0.09   \n",
      "Iteration   40: Cost     0.09   \n",
      "Iteration   50: Cost     0.09   \n",
      "Iteration   60: Cost     0.09   \n",
      "Iteration   70: Cost     0.09   \n",
      "Iteration   80: Cost     0.09   \n",
      "Iteration   90: Cost     0.09   \n",
      "Iteration   99: Cost     0.09   \n"
     ]
    }
   ],
   "source": [
    "lr.fit(X_train, y_train[: , 1], alpha=0.000003, iterations=100, lambda_ = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "ed305b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 99.062500\n"
     ]
    }
   ],
   "source": [
    "#Compute accuracy on our training set\n",
    "p = lr.predict(X_train, 0.5)\n",
    "print('Train Accuracy: %f'%(np.mean(p == y_train[:, 1]) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907531ca",
   "metadata": {},
   "source": [
    "#### Testing..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d88b883",
   "metadata": {},
   "source": [
    "# Support Multiclass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "4b45f4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultipleLogisticRegression:\n",
    "\n",
    "    def __init__(self):\n",
    "    \n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        \n",
    "        # C: number of categories for Y\n",
    "        # -> create C models and train them each one at teh time\n",
    "        self.c = None\n",
    "        \n",
    "        self.models = []\n",
    "        \n",
    "    def softmax(self, X):\n",
    "        \"\"\"\n",
    "        takes the predicted values from the sub_models\n",
    "        \"\"\"\n",
    "        pred = []\n",
    "        for i in range(self.c):\n",
    "            pred.append(self.models[i].predict(X))\n",
    "        pred = np.array(pred)\n",
    "        res = []\n",
    "        for row in pred.T[0]:\n",
    "            # for each row (student), return the column (house) with the highest probability\n",
    "            res.append(np.argmax(row / np.sum(row)))\n",
    "        res = np.array(res)\n",
    "        return res\n",
    "    \n",
    "    def accuracy(self, pred, true_labels):\n",
    "        \"\"\"\n",
    "        calculate the accuracy of the model prediction given the true labels\n",
    "        \"\"\"\n",
    "        if pred.shape != true_labels.shape:\n",
    "            print('Error in the shape of predictions and true_labels')\n",
    "            return \n",
    "        accuracy = np.mean(pred == true_labels) * 100\n",
    "        print('Accuracy: %f'%(accuracy))\n",
    "        return accuracy\n",
    "    \n",
    "    def score_matrix(self, pred, true_labels):\n",
    "        \"\"\"\n",
    "        calculate scores matrix\n",
    "        \"\"\"\n",
    "        # Calculate overall precision\n",
    "        score_matrix = np.zeros((self.c, self.c))\n",
    "        for i in range(0, len(pred)):\n",
    "            pred_label = pred[i]\n",
    "            true_label = answer[i]\n",
    "            score_matrix[pred_label][true_label] += 1\n",
    "            \n",
    "        precision_per_class = []\n",
    "        for i in range(self.c):\n",
    "            precision_per_class.append(score_matrix[i][i] / score_matrix[i].sum())\n",
    "        \n",
    "        return score_matrix, precision_per_class\n",
    "        \n",
    "    def fit(self, X, y, alpha=0.0001, iterations=10000, show_every=None, lambda_= 1):\n",
    "        \"\"\"\n",
    "        setup attributes and apply training\n",
    "        \"\"\"\n",
    "        \n",
    "        # train data and label\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "        # m: number of observations\n",
    "        self.m, self.n = X.shape\n",
    "        \n",
    "        # c: number of category\n",
    "        self.c = y.shape[1]\n",
    "        \n",
    "        \n",
    "        if show_every == None:\n",
    "            if (iterations <= 10):\n",
    "                show_every = 1\n",
    "            else:\n",
    "                show_every = iterations // 10 \n",
    "        \n",
    "        # init models\n",
    "        if not self.models:\n",
    "            for i in range(self.c):\n",
    "                self.models.append(LogisticRegression())\n",
    "        \n",
    "        # train models\n",
    "        decision_boundary = 0.5\n",
    "        for i in range(self.c):\n",
    "            model = self.models[i]\n",
    "            print(f\"training model {i+1} with alpha= {alpha}\")\n",
    "            model.fit(X, y[: , i], alpha=alpha, iterations=iterations, lambda_ = lambda_)\n",
    "            p = model.predict(X, decision_boundary)\n",
    "            print('Train Accuracy: %f'%(np.mean(p == y[:, i]) * 100))\n",
    "            print()\n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        return self.softmax(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "5ac94605",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = MultipleLogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "1fbc30fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training model 1 with alpha= 0.0003\n",
      "Iteration    0: Cost     0.44   \n",
      "Iteration 1000: Cost     0.05   \n",
      "Iteration 2000: Cost     0.04   \n",
      "Iteration 3000: Cost     0.04   \n",
      "Iteration 4000: Cost     0.04   \n",
      "Iteration 5000: Cost     0.04   \n",
      "Iteration 6000: Cost     0.04   \n",
      "Iteration 7000: Cost     0.04   \n",
      "Iteration 8000: Cost     0.04   \n",
      "Iteration 9000: Cost     0.04   \n",
      "Iteration 9999: Cost     0.04   \n",
      "Train Accuracy: 99.187500\n",
      "\n",
      "training model 2 with alpha= 0.0003\n",
      "Iteration    0: Cost     0.53   \n",
      "Iteration 1000: Cost     0.06   \n",
      "Iteration 2000: Cost     0.06   \n",
      "Iteration 3000: Cost     0.06   \n",
      "Iteration 4000: Cost     0.06   \n",
      "Iteration 5000: Cost     0.06   \n",
      "Iteration 6000: Cost     0.06   \n",
      "Iteration 7000: Cost     0.06   \n",
      "Iteration 8000: Cost     0.06   \n",
      "Iteration 9000: Cost     0.06   \n",
      "Iteration 9999: Cost     0.06   \n",
      "Train Accuracy: 99.000000\n",
      "\n",
      "training model 3 with alpha= 0.0003\n",
      "Iteration    0: Cost     0.46   \n",
      "Iteration 1000: Cost     0.07   \n",
      "Iteration 2000: Cost     0.07   \n",
      "Iteration 3000: Cost     0.07   \n",
      "Iteration 4000: Cost     0.07   \n",
      "Iteration 5000: Cost     0.07   \n",
      "Iteration 6000: Cost     0.07   \n",
      "Iteration 7000: Cost     0.07   \n",
      "Iteration 8000: Cost     0.07   \n",
      "Iteration 9000: Cost     0.07   \n",
      "Iteration 9999: Cost     0.07   \n",
      "Train Accuracy: 98.812500\n",
      "\n",
      "training model 4 with alpha= 0.0003\n",
      "Iteration    0: Cost     0.51   \n",
      "Iteration 1000: Cost     0.05   \n",
      "Iteration 2000: Cost     0.05   \n",
      "Iteration 3000: Cost     0.05   \n",
      "Iteration 4000: Cost     0.05   \n",
      "Iteration 5000: Cost     0.05   \n",
      "Iteration 6000: Cost     0.05   \n",
      "Iteration 7000: Cost     0.05   \n",
      "Iteration 8000: Cost     0.05   \n",
      "Iteration 9000: Cost     0.05   \n",
      "Iteration 9999: Cost     0.05   \n",
      "Train Accuracy: 99.312500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models.fit(X_train, y_train, alpha=0.0003, iterations=10000, lambda_=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "52307a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training model 1 with alpha= 3e-05\n",
      "Iteration    0: Cost     0.04   \n",
      "Iteration 1000: Cost     0.04   \n",
      "Iteration 2000: Cost     0.04   \n",
      "Iteration 3000: Cost     0.04   \n",
      "Iteration 4000: Cost     0.04   \n",
      "Iteration 5000: Cost     0.04   \n",
      "Iteration 6000: Cost     0.04   \n",
      "Iteration 7000: Cost     0.04   \n",
      "Iteration 8000: Cost     0.04   \n",
      "Iteration 9000: Cost     0.04   \n",
      "Iteration 9999: Cost     0.04   \n",
      "Train Accuracy: 99.187500\n",
      "\n",
      "training model 2 with alpha= 3e-05\n",
      "Iteration    0: Cost     0.06   \n",
      "Iteration 1000: Cost     0.06   \n",
      "Iteration 2000: Cost     0.06   \n",
      "Iteration 3000: Cost     0.06   \n",
      "Iteration 4000: Cost     0.06   \n",
      "Iteration 5000: Cost     0.06   \n",
      "Iteration 6000: Cost     0.06   \n",
      "Iteration 7000: Cost     0.06   \n",
      "Iteration 8000: Cost     0.06   \n",
      "Iteration 9000: Cost     0.06   \n",
      "Iteration 9999: Cost     0.06   \n",
      "Train Accuracy: 99.000000\n",
      "\n",
      "training model 3 with alpha= 3e-05\n",
      "Iteration    0: Cost     0.07   \n",
      "Iteration 1000: Cost     0.07   \n",
      "Iteration 2000: Cost     0.07   \n",
      "Iteration 3000: Cost     0.07   \n",
      "Iteration 4000: Cost     0.07   \n",
      "Iteration 5000: Cost     0.07   \n",
      "Iteration 6000: Cost     0.07   \n",
      "Iteration 7000: Cost     0.07   \n",
      "Iteration 8000: Cost     0.07   \n",
      "Iteration 9000: Cost     0.07   \n",
      "Iteration 9999: Cost     0.07   \n",
      "Train Accuracy: 98.812500\n",
      "\n",
      "training model 4 with alpha= 3e-05\n",
      "Iteration    0: Cost     0.05   \n",
      "Iteration 1000: Cost     0.05   \n",
      "Iteration 2000: Cost     0.05   \n",
      "Iteration 3000: Cost     0.05   \n",
      "Iteration 4000: Cost     0.05   \n",
      "Iteration 5000: Cost     0.05   \n",
      "Iteration 6000: Cost     0.05   \n",
      "Iteration 7000: Cost     0.05   \n",
      "Iteration 8000: Cost     0.05   \n",
      "Iteration 9000: Cost     0.05   \n",
      "Iteration 9999: Cost     0.05   \n",
      "Train Accuracy: 99.312500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models.fit(X_train, y_train, alpha=0.00003, iterations=10000, lambda_=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "7ab2454e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.array(models.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "1025ec9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 2, ..., 0, 1, 1])"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02213c9",
   "metadata": {},
   "source": [
    "### Get true labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "bbbfbb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = []\n",
    "for row in y_train:\n",
    "    answer.append(np.argmax(row))\n",
    "answer = np.array(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "eaa9215f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 98.187500\n"
     ]
    }
   ],
   "source": [
    "#Compute accuracy on our training set\n",
    "print('Train Accuracy: %f'%(np.mean(pred == answer) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5295296f",
   "metadata": {},
   "source": [
    "### Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "f314c411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://miro.medium.com/max/1400/1*eS4zV29dd9vFo0KQSk8s7w.png\" width=\"500\" height=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://miro.medium.com/max/1400/1*eS4zV29dd9vFo0KQSk8s7w.png\", width=500, height=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "e9b8554a",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_matrix, precision = models.score_matrix(pred, answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "cf362880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[318.,   2.,   2.,   0.],\n",
       "       [  4., 525.,   4.,   3.],\n",
       "       [  5.,   1., 435.,   5.],\n",
       "       [  0.,   1.,   2., 293.]])"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "a2c6b723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9875776397515528,\n",
       " 0.9794776119402985,\n",
       " 0.9753363228699552,\n",
       " 0.9898648648648649]"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "186756dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global precision =  0.9830641098566678\n"
     ]
    }
   ],
   "source": [
    "print(\"global precision = \", sum(precision)/len(precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377837d6",
   "metadata": {},
   "source": [
    "## Trying prediction on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5298c8",
   "metadata": {},
   "source": [
    "Reconvert Array of Integer (category number) to 4 arrays of 0and1 for each House"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d472e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(test)\n",
    "test_pred = models.softmax(X_test)\n",
    "test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbd6f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = np.zeros((len(test_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c934ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(test_pred, columns=['Hogwarts House'])\n",
    "target_columns = ['Gryffindor', 'Hufflepuff', 'Ravenclaw', 'Slytherin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a55c4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df)):\n",
    "    idx = df['Hogwarts House'][i]\n",
    "    df['Hogwarts House'][i] = target_columns[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426f393c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Index'] = df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ce2ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfea2fe",
   "metadata": {},
   "source": [
    "### Save to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096a10c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('prediction.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728e50c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "afd1ddcb",
   "metadata": {},
   "source": [
    "# Test library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "0a82cda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "6d218819",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(solver='liblinear', random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "108c7599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(random_state=0, solver=&#x27;liblinear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(random_state=0, solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(random_state=0, solver='liblinear')"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "5d2ce77f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.24713076, -1.69407535,  1.12325885,  0.02835036,  0.43603106,\n",
       "        -0.20803818, -0.71969753, -0.06729304, -0.00727891,  1.63294209]])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eeda6bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
